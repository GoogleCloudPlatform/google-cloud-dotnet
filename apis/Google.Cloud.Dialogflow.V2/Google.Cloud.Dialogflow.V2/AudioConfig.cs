// <auto-generated>
//     Generated by the protocol buffer compiler.  DO NOT EDIT!
//     source: google/cloud/dialogflow/v2/audio_config.proto
// </auto-generated>
#pragma warning disable 1591, 0612, 3021
#region Designer generated code

using pb = global::Google.Protobuf;
using pbc = global::Google.Protobuf.Collections;
using pbr = global::Google.Protobuf.Reflection;
using scg = global::System.Collections.Generic;
namespace Google.Cloud.Dialogflow.V2 {

  /// <summary>Holder for reflection information generated from google/cloud/dialogflow/v2/audio_config.proto</summary>
  public static partial class AudioConfigReflection {

    #region Descriptor
    /// <summary>File descriptor for google/cloud/dialogflow/v2/audio_config.proto</summary>
    public static pbr::FileDescriptor Descriptor {
      get { return descriptor; }
    }
    private static pbr::FileDescriptor descriptor;

    static AudioConfigReflection() {
      byte[] descriptorData = global::System.Convert.FromBase64String(
          string.Concat(
            "Ci1nb29nbGUvY2xvdWQvZGlhbG9nZmxvdy92Mi9hdWRpb19jb25maWcucHJv",
            "dG8SGmdvb2dsZS5jbG91ZC5kaWFsb2dmbG93LnYyGh9nb29nbGUvYXBpL2Zp",
            "ZWxkX2JlaGF2aW9yLnByb3RvGhlnb29nbGUvYXBpL3Jlc291cmNlLnByb3Rv",
            "Gh5nb29nbGUvcHJvdG9idWYvZHVyYXRpb24ucHJvdG8aHGdvb2dsZS9hcGkv",
            "YW5ub3RhdGlvbnMucHJvdG8i/gEKEElucHV0QXVkaW9Db25maWcSQQoOYXVk",
            "aW9fZW5jb2RpbmcYASABKA4yKS5nb29nbGUuY2xvdWQuZGlhbG9nZmxvdy52",
            "Mi5BdWRpb0VuY29kaW5nEhkKEXNhbXBsZV9yYXRlX2hlcnR6GAIgASgFEhUK",
            "DWxhbmd1YWdlX2NvZGUYAyABKAkSFAoMcGhyYXNlX2hpbnRzGAQgAygJEkUK",
            "DW1vZGVsX3ZhcmlhbnQYCiABKA4yLi5nb29nbGUuY2xvdWQuZGlhbG9nZmxv",
            "dy52Mi5TcGVlY2hNb2RlbFZhcmlhbnQSGAoQc2luZ2xlX3V0dGVyYW5jZRgI",
            "IAEoCCJmChRWb2ljZVNlbGVjdGlvblBhcmFtcxIMCgRuYW1lGAEgASgJEkAK",
            "C3NzbWxfZ2VuZGVyGAIgASgOMisuZ29vZ2xlLmNsb3VkLmRpYWxvZ2Zsb3cu",
            "djIuU3NtbFZvaWNlR2VuZGVyIrMBChZTeW50aGVzaXplU3BlZWNoQ29uZmln",
            "EhUKDXNwZWFraW5nX3JhdGUYASABKAESDQoFcGl0Y2gYAiABKAESFgoOdm9s",
            "dW1lX2dhaW5fZGIYAyABKAESGgoSZWZmZWN0c19wcm9maWxlX2lkGAUgAygJ",
            "Ej8KBXZvaWNlGAQgASgLMjAuZ29vZ2xlLmNsb3VkLmRpYWxvZ2Zsb3cudjIu",
            "Vm9pY2VTZWxlY3Rpb25QYXJhbXMizQEKEU91dHB1dEF1ZGlvQ29uZmlnEkcK",
            "DmF1ZGlvX2VuY29kaW5nGAEgASgOMi8uZ29vZ2xlLmNsb3VkLmRpYWxvZ2Zs",
            "b3cudjIuT3V0cHV0QXVkaW9FbmNvZGluZxIZChFzYW1wbGVfcmF0ZV9oZXJ0",
            "ehgCIAEoBRJUChhzeW50aGVzaXplX3NwZWVjaF9jb25maWcYAyABKAsyMi5n",
            "b29nbGUuY2xvdWQuZGlhbG9nZmxvdy52Mi5TeW50aGVzaXplU3BlZWNoQ29u",
            "ZmlnKvsBCg1BdWRpb0VuY29kaW5nEh4KGkFVRElPX0VOQ09ESU5HX1VOU1BF",
            "Q0lGSUVEEAASHAoYQVVESU9fRU5DT0RJTkdfTElORUFSXzE2EAESFwoTQVVE",
            "SU9fRU5DT0RJTkdfRkxBQxACEhgKFEFVRElPX0VOQ09ESU5HX01VTEFXEAMS",
            "FgoSQVVESU9fRU5DT0RJTkdfQU1SEAQSGQoVQVVESU9fRU5DT0RJTkdfQU1S",
            "X1dCEAUSGwoXQVVESU9fRU5DT0RJTkdfT0dHX09QVVMQBhIpCiVBVURJT19F",
            "TkNPRElOR19TUEVFWF9XSVRIX0hFQURFUl9CWVRFEAcqdgoSU3BlZWNoTW9k",
            "ZWxWYXJpYW50EiQKIFNQRUVDSF9NT0RFTF9WQVJJQU5UX1VOU1BFQ0lGSUVE",
            "EAASFgoSVVNFX0JFU1RfQVZBSUxBQkxFEAESEAoMVVNFX1NUQU5EQVJEEAIS",
            "EAoMVVNFX0VOSEFOQ0VEEAMqjQEKD1NzbWxWb2ljZUdlbmRlchIhCh1TU01M",
            "X1ZPSUNFX0dFTkRFUl9VTlNQRUNJRklFRBAAEhoKFlNTTUxfVk9JQ0VfR0VO",
            "REVSX01BTEUQARIcChhTU01MX1ZPSUNFX0dFTkRFUl9GRU1BTEUQAhIdChlT",
            "U01MX1ZPSUNFX0dFTkRFUl9ORVVUUkFMEAMqpAEKE091dHB1dEF1ZGlvRW5j",
            "b2RpbmcSJQohT1VUUFVUX0FVRElPX0VOQ09ESU5HX1VOU1BFQ0lGSUVEEAAS",
            "IwofT1VUUFVUX0FVRElPX0VOQ09ESU5HX0xJTkVBUl8xNhABEh0KGU9VVFBV",
            "VF9BVURJT19FTkNPRElOR19NUDMQAhIiCh5PVVRQVVRfQVVESU9fRU5DT0RJ",
            "TkdfT0dHX09QVVMQA0KfAQoeY29tLmdvb2dsZS5jbG91ZC5kaWFsb2dmbG93",
            "LnYyQhBBdWRpb0NvbmZpZ1Byb3RvUAFaRGdvb2dsZS5nb2xhbmcub3JnL2dl",
            "bnByb3RvL2dvb2dsZWFwaXMvY2xvdWQvZGlhbG9nZmxvdy92MjtkaWFsb2dm",
            "bG93+AEBogICREaqAhpHb29nbGUuQ2xvdWQuRGlhbG9nZmxvdy5WMmIGcHJv",
            "dG8z"));
      descriptor = pbr::FileDescriptor.FromGeneratedCode(descriptorData,
          new pbr::FileDescriptor[] { global::Google.Api.FieldBehaviorReflection.Descriptor, global::Google.Api.ResourceReflection.Descriptor, global::Google.Protobuf.WellKnownTypes.DurationReflection.Descriptor, global::Google.Api.AnnotationsReflection.Descriptor, },
          new pbr::GeneratedClrTypeInfo(new[] {typeof(global::Google.Cloud.Dialogflow.V2.AudioEncoding), typeof(global::Google.Cloud.Dialogflow.V2.SpeechModelVariant), typeof(global::Google.Cloud.Dialogflow.V2.SsmlVoiceGender), typeof(global::Google.Cloud.Dialogflow.V2.OutputAudioEncoding), }, new pbr::GeneratedClrTypeInfo[] {
            new pbr::GeneratedClrTypeInfo(typeof(global::Google.Cloud.Dialogflow.V2.InputAudioConfig), global::Google.Cloud.Dialogflow.V2.InputAudioConfig.Parser, new[]{ "AudioEncoding", "SampleRateHertz", "LanguageCode", "PhraseHints", "ModelVariant", "SingleUtterance" }, null, null, null),
            new pbr::GeneratedClrTypeInfo(typeof(global::Google.Cloud.Dialogflow.V2.VoiceSelectionParams), global::Google.Cloud.Dialogflow.V2.VoiceSelectionParams.Parser, new[]{ "Name", "SsmlGender" }, null, null, null),
            new pbr::GeneratedClrTypeInfo(typeof(global::Google.Cloud.Dialogflow.V2.SynthesizeSpeechConfig), global::Google.Cloud.Dialogflow.V2.SynthesizeSpeechConfig.Parser, new[]{ "SpeakingRate", "Pitch", "VolumeGainDb", "EffectsProfileId", "Voice" }, null, null, null),
            new pbr::GeneratedClrTypeInfo(typeof(global::Google.Cloud.Dialogflow.V2.OutputAudioConfig), global::Google.Cloud.Dialogflow.V2.OutputAudioConfig.Parser, new[]{ "AudioEncoding", "SampleRateHertz", "SynthesizeSpeechConfig" }, null, null, null)
          }));
    }
    #endregion

  }
  #region Enums
  /// <summary>
  /// Audio encoding of the audio content sent in the conversational query request.
  /// Refer to the
  /// [Cloud Speech API
  /// documentation](https://cloud.google.com/speech-to-text/docs/basics) for more
  /// details.
  /// </summary>
  public enum AudioEncoding {
    /// <summary>
    /// Not specified.
    /// </summary>
    [pbr::OriginalName("AUDIO_ENCODING_UNSPECIFIED")] Unspecified = 0,
    /// <summary>
    /// Uncompressed 16-bit signed little-endian samples (Linear PCM).
    /// </summary>
    [pbr::OriginalName("AUDIO_ENCODING_LINEAR_16")] Linear16 = 1,
    /// <summary>
    /// [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
    /// Codec) is the recommended encoding because it is lossless (therefore
    /// recognition is not compromised) and requires only about half the
    /// bandwidth of `LINEAR16`. `FLAC` stream encoding supports 16-bit and
    /// 24-bit samples, however, not all fields in `STREAMINFO` are supported.
    /// </summary>
    [pbr::OriginalName("AUDIO_ENCODING_FLAC")] Flac = 2,
    /// <summary>
    /// 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
    /// </summary>
    [pbr::OriginalName("AUDIO_ENCODING_MULAW")] Mulaw = 3,
    /// <summary>
    /// Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
    /// </summary>
    [pbr::OriginalName("AUDIO_ENCODING_AMR")] Amr = 4,
    /// <summary>
    /// Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
    /// </summary>
    [pbr::OriginalName("AUDIO_ENCODING_AMR_WB")] AmrWb = 5,
    /// <summary>
    /// Opus encoded audio frames in Ogg container
    /// ([OggOpus](https://wiki.xiph.org/OggOpus)).
    /// `sample_rate_hertz` must be 16000.
    /// </summary>
    [pbr::OriginalName("AUDIO_ENCODING_OGG_OPUS")] OggOpus = 6,
    /// <summary>
    /// Although the use of lossy encodings is not recommended, if a very low
    /// bitrate encoding is required, `OGG_OPUS` is highly preferred over
    /// Speex encoding. The [Speex](https://speex.org/) encoding supported by
    /// Dialogflow API has a header byte in each block, as in MIME type
    /// `audio/x-speex-with-header-byte`.
    /// It is a variant of the RTP Speex encoding defined in
    /// [RFC 5574](https://tools.ietf.org/html/rfc5574).
    /// The stream is a sequence of blocks, one block per RTP packet. Each block
    /// starts with a byte containing the length of the block, in bytes, followed
    /// by one or more frames of Speex data, padded to an integral number of
    /// bytes (octets) as specified in RFC 5574. In other words, each RTP header
    /// is replaced with a single byte containing the block length. Only Speex
    /// wideband is supported. `sample_rate_hertz` must be 16000.
    /// </summary>
    [pbr::OriginalName("AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE")] SpeexWithHeaderByte = 7,
  }

  /// <summary>
  /// Variant of the specified [Speech model][google.cloud.dialogflow.v2.InputAudioConfig.model] to use.
  ///
  /// See the [Cloud Speech
  /// documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
  /// for which models have different variants. For example, the "phone_call" model
  /// has both a standard and an enhanced variant. When you use an enhanced model,
  /// you will generally receive higher quality results than for a standard model.
  /// </summary>
  public enum SpeechModelVariant {
    /// <summary>
    /// No model variant specified. In this case Dialogflow defaults to
    /// USE_BEST_AVAILABLE.
    /// </summary>
    [pbr::OriginalName("SPEECH_MODEL_VARIANT_UNSPECIFIED")] Unspecified = 0,
    /// <summary>
    /// Use the best available variant of the [Speech
    /// model][InputAudioConfig.model] that the caller is eligible for.
    ///
    /// Please see the [Dialogflow
    /// docs](https://cloud.google.com/dialogflow/docs/data-logging) for
    /// how to make your project eligible for enhanced models.
    /// </summary>
    [pbr::OriginalName("USE_BEST_AVAILABLE")] UseBestAvailable = 1,
    /// <summary>
    /// Use standard model variant even if an enhanced model is available.  See the
    /// [Cloud Speech
    /// documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
    /// for details about enhanced models.
    /// </summary>
    [pbr::OriginalName("USE_STANDARD")] UseStandard = 2,
    /// <summary>
    /// Use an enhanced model variant:
    ///
    /// * If an enhanced variant does not exist for the given
    ///   [model][google.cloud.dialogflow.v2.InputAudioConfig.model] and request language, Dialogflow falls
    ///   back to the standard variant.
    ///
    ///   The [Cloud Speech
    ///   documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
    ///   describes which models have enhanced variants.
    ///
    /// * If the API caller isn't eligible for enhanced models, Dialogflow returns
    ///   an error. Please see the [Dialogflow
    ///   docs](https://cloud.google.com/dialogflow/docs/data-logging)
    ///   for how to make your project eligible.
    /// </summary>
    [pbr::OriginalName("USE_ENHANCED")] UseEnhanced = 3,
  }

  /// <summary>
  /// Gender of the voice as described in
  /// [SSML voice element](https://www.w3.org/TR/speech-synthesis11/#edef_voice).
  /// </summary>
  public enum SsmlVoiceGender {
    /// <summary>
    /// An unspecified gender, which means that the client doesn't care which
    /// gender the selected voice will have.
    /// </summary>
    [pbr::OriginalName("SSML_VOICE_GENDER_UNSPECIFIED")] Unspecified = 0,
    /// <summary>
    /// A male voice.
    /// </summary>
    [pbr::OriginalName("SSML_VOICE_GENDER_MALE")] Male = 1,
    /// <summary>
    /// A female voice.
    /// </summary>
    [pbr::OriginalName("SSML_VOICE_GENDER_FEMALE")] Female = 2,
    /// <summary>
    /// A gender-neutral voice.
    /// </summary>
    [pbr::OriginalName("SSML_VOICE_GENDER_NEUTRAL")] Neutral = 3,
  }

  /// <summary>
  /// Audio encoding of the output audio format in Text-To-Speech.
  /// </summary>
  public enum OutputAudioEncoding {
    /// <summary>
    /// Not specified.
    /// </summary>
    [pbr::OriginalName("OUTPUT_AUDIO_ENCODING_UNSPECIFIED")] Unspecified = 0,
    /// <summary>
    /// Uncompressed 16-bit signed little-endian samples (Linear PCM).
    /// Audio content returned as LINEAR16 also contains a WAV header.
    /// </summary>
    [pbr::OriginalName("OUTPUT_AUDIO_ENCODING_LINEAR_16")] Linear16 = 1,
    /// <summary>
    /// MP3 audio.
    /// </summary>
    [pbr::OriginalName("OUTPUT_AUDIO_ENCODING_MP3")] Mp3 = 2,
    /// <summary>
    /// Opus encoded audio wrapped in an ogg container. The result will be a
    /// file which can be played natively on Android, and in browsers (at least
    /// Chrome and Firefox). The quality of the encoding is considerably higher
    /// than MP3 while using approximately the same bitrate.
    /// </summary>
    [pbr::OriginalName("OUTPUT_AUDIO_ENCODING_OGG_OPUS")] OggOpus = 3,
  }

  #endregion

  #region Messages
  /// <summary>
  /// Instructs the speech recognizer how to process the audio content.
  /// </summary>
  public sealed partial class InputAudioConfig : pb::IMessage<InputAudioConfig> {
    private static readonly pb::MessageParser<InputAudioConfig> _parser = new pb::MessageParser<InputAudioConfig>(() => new InputAudioConfig());
    private pb::UnknownFieldSet _unknownFields;
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public static pb::MessageParser<InputAudioConfig> Parser { get { return _parser; } }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public static pbr::MessageDescriptor Descriptor {
      get { return global::Google.Cloud.Dialogflow.V2.AudioConfigReflection.Descriptor.MessageTypes[0]; }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    pbr::MessageDescriptor pb::IMessage.Descriptor {
      get { return Descriptor; }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public InputAudioConfig() {
      OnConstruction();
    }

    partial void OnConstruction();

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public InputAudioConfig(InputAudioConfig other) : this() {
      audioEncoding_ = other.audioEncoding_;
      sampleRateHertz_ = other.sampleRateHertz_;
      languageCode_ = other.languageCode_;
      phraseHints_ = other.phraseHints_.Clone();
      modelVariant_ = other.modelVariant_;
      singleUtterance_ = other.singleUtterance_;
      _unknownFields = pb::UnknownFieldSet.Clone(other._unknownFields);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public InputAudioConfig Clone() {
      return new InputAudioConfig(this);
    }

    /// <summary>Field number for the "audio_encoding" field.</summary>
    public const int AudioEncodingFieldNumber = 1;
    private global::Google.Cloud.Dialogflow.V2.AudioEncoding audioEncoding_ = 0;
    /// <summary>
    /// Required. Audio encoding of the audio content to process.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public global::Google.Cloud.Dialogflow.V2.AudioEncoding AudioEncoding {
      get { return audioEncoding_; }
      set {
        audioEncoding_ = value;
      }
    }

    /// <summary>Field number for the "sample_rate_hertz" field.</summary>
    public const int SampleRateHertzFieldNumber = 2;
    private int sampleRateHertz_;
    /// <summary>
    /// Required. Sample rate (in Hertz) of the audio content sent in the query.
    /// Refer to
    /// [Cloud Speech API
    /// documentation](https://cloud.google.com/speech-to-text/docs/basics) for
    /// more details.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public int SampleRateHertz {
      get { return sampleRateHertz_; }
      set {
        sampleRateHertz_ = value;
      }
    }

    /// <summary>Field number for the "language_code" field.</summary>
    public const int LanguageCodeFieldNumber = 3;
    private string languageCode_ = "";
    /// <summary>
    /// Required. The language of the supplied audio. Dialogflow does not do
    /// translations. See [Language
    /// Support](https://cloud.google.com/dialogflow/docs/reference/language)
    /// for a list of the currently supported language codes. Note that queries in
    /// the same session do not necessarily need to specify the same language.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public string LanguageCode {
      get { return languageCode_; }
      set {
        languageCode_ = pb::ProtoPreconditions.CheckNotNull(value, "value");
      }
    }

    /// <summary>Field number for the "phrase_hints" field.</summary>
    public const int PhraseHintsFieldNumber = 4;
    private static readonly pb::FieldCodec<string> _repeated_phraseHints_codec
        = pb::FieldCodec.ForString(34);
    private readonly pbc::RepeatedField<string> phraseHints_ = new pbc::RepeatedField<string>();
    /// <summary>
    /// Optional. A list of strings containing words and phrases that the speech
    /// recognizer should recognize with higher likelihood.
    ///
    /// See [the Cloud Speech
    /// documentation](https://cloud.google.com/speech-to-text/docs/basics#phrase-hints)
    /// for more details.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public pbc::RepeatedField<string> PhraseHints {
      get { return phraseHints_; }
    }

    /// <summary>Field number for the "model_variant" field.</summary>
    public const int ModelVariantFieldNumber = 10;
    private global::Google.Cloud.Dialogflow.V2.SpeechModelVariant modelVariant_ = 0;
    /// <summary>
    /// Optional. Which variant of the [Speech model][google.cloud.dialogflow.v2.InputAudioConfig.model] to use.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public global::Google.Cloud.Dialogflow.V2.SpeechModelVariant ModelVariant {
      get { return modelVariant_; }
      set {
        modelVariant_ = value;
      }
    }

    /// <summary>Field number for the "single_utterance" field.</summary>
    public const int SingleUtteranceFieldNumber = 8;
    private bool singleUtterance_;
    /// <summary>
    /// Optional. If `false` (default), recognition does not cease until the
    /// client closes the stream.
    /// If `true`, the recognizer will detect a single spoken utterance in input
    /// audio. Recognition ceases when it detects the audio's voice has
    /// stopped or paused. In this case, once a detected intent is received, the
    /// client should close the stream and start a new request with a new stream as
    /// needed.
    /// Note: This setting is relevant only for streaming methods.
    /// Note: When specified, InputAudioConfig.single_utterance takes precedence
    /// over StreamingDetectIntentRequest.single_utterance.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public bool SingleUtterance {
      get { return singleUtterance_; }
      set {
        singleUtterance_ = value;
      }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public override bool Equals(object other) {
      return Equals(other as InputAudioConfig);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public bool Equals(InputAudioConfig other) {
      if (ReferenceEquals(other, null)) {
        return false;
      }
      if (ReferenceEquals(other, this)) {
        return true;
      }
      if (AudioEncoding != other.AudioEncoding) return false;
      if (SampleRateHertz != other.SampleRateHertz) return false;
      if (LanguageCode != other.LanguageCode) return false;
      if(!phraseHints_.Equals(other.phraseHints_)) return false;
      if (ModelVariant != other.ModelVariant) return false;
      if (SingleUtterance != other.SingleUtterance) return false;
      return Equals(_unknownFields, other._unknownFields);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public override int GetHashCode() {
      int hash = 1;
      if (AudioEncoding != 0) hash ^= AudioEncoding.GetHashCode();
      if (SampleRateHertz != 0) hash ^= SampleRateHertz.GetHashCode();
      if (LanguageCode.Length != 0) hash ^= LanguageCode.GetHashCode();
      hash ^= phraseHints_.GetHashCode();
      if (ModelVariant != 0) hash ^= ModelVariant.GetHashCode();
      if (SingleUtterance != false) hash ^= SingleUtterance.GetHashCode();
      if (_unknownFields != null) {
        hash ^= _unknownFields.GetHashCode();
      }
      return hash;
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public override string ToString() {
      return pb::JsonFormatter.ToDiagnosticString(this);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public void WriteTo(pb::CodedOutputStream output) {
      if (AudioEncoding != 0) {
        output.WriteRawTag(8);
        output.WriteEnum((int) AudioEncoding);
      }
      if (SampleRateHertz != 0) {
        output.WriteRawTag(16);
        output.WriteInt32(SampleRateHertz);
      }
      if (LanguageCode.Length != 0) {
        output.WriteRawTag(26);
        output.WriteString(LanguageCode);
      }
      phraseHints_.WriteTo(output, _repeated_phraseHints_codec);
      if (SingleUtterance != false) {
        output.WriteRawTag(64);
        output.WriteBool(SingleUtterance);
      }
      if (ModelVariant != 0) {
        output.WriteRawTag(80);
        output.WriteEnum((int) ModelVariant);
      }
      if (_unknownFields != null) {
        _unknownFields.WriteTo(output);
      }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public int CalculateSize() {
      int size = 0;
      if (AudioEncoding != 0) {
        size += 1 + pb::CodedOutputStream.ComputeEnumSize((int) AudioEncoding);
      }
      if (SampleRateHertz != 0) {
        size += 1 + pb::CodedOutputStream.ComputeInt32Size(SampleRateHertz);
      }
      if (LanguageCode.Length != 0) {
        size += 1 + pb::CodedOutputStream.ComputeStringSize(LanguageCode);
      }
      size += phraseHints_.CalculateSize(_repeated_phraseHints_codec);
      if (ModelVariant != 0) {
        size += 1 + pb::CodedOutputStream.ComputeEnumSize((int) ModelVariant);
      }
      if (SingleUtterance != false) {
        size += 1 + 1;
      }
      if (_unknownFields != null) {
        size += _unknownFields.CalculateSize();
      }
      return size;
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public void MergeFrom(InputAudioConfig other) {
      if (other == null) {
        return;
      }
      if (other.AudioEncoding != 0) {
        AudioEncoding = other.AudioEncoding;
      }
      if (other.SampleRateHertz != 0) {
        SampleRateHertz = other.SampleRateHertz;
      }
      if (other.LanguageCode.Length != 0) {
        LanguageCode = other.LanguageCode;
      }
      phraseHints_.Add(other.phraseHints_);
      if (other.ModelVariant != 0) {
        ModelVariant = other.ModelVariant;
      }
      if (other.SingleUtterance != false) {
        SingleUtterance = other.SingleUtterance;
      }
      _unknownFields = pb::UnknownFieldSet.MergeFrom(_unknownFields, other._unknownFields);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public void MergeFrom(pb::CodedInputStream input) {
      uint tag;
      while ((tag = input.ReadTag()) != 0) {
        switch(tag) {
          default:
            _unknownFields = pb::UnknownFieldSet.MergeFieldFrom(_unknownFields, input);
            break;
          case 8: {
            AudioEncoding = (global::Google.Cloud.Dialogflow.V2.AudioEncoding) input.ReadEnum();
            break;
          }
          case 16: {
            SampleRateHertz = input.ReadInt32();
            break;
          }
          case 26: {
            LanguageCode = input.ReadString();
            break;
          }
          case 34: {
            phraseHints_.AddEntriesFrom(input, _repeated_phraseHints_codec);
            break;
          }
          case 64: {
            SingleUtterance = input.ReadBool();
            break;
          }
          case 80: {
            ModelVariant = (global::Google.Cloud.Dialogflow.V2.SpeechModelVariant) input.ReadEnum();
            break;
          }
        }
      }
    }

  }

  /// <summary>
  /// Description of which voice to use for speech synthesis.
  /// </summary>
  public sealed partial class VoiceSelectionParams : pb::IMessage<VoiceSelectionParams> {
    private static readonly pb::MessageParser<VoiceSelectionParams> _parser = new pb::MessageParser<VoiceSelectionParams>(() => new VoiceSelectionParams());
    private pb::UnknownFieldSet _unknownFields;
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public static pb::MessageParser<VoiceSelectionParams> Parser { get { return _parser; } }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public static pbr::MessageDescriptor Descriptor {
      get { return global::Google.Cloud.Dialogflow.V2.AudioConfigReflection.Descriptor.MessageTypes[1]; }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    pbr::MessageDescriptor pb::IMessage.Descriptor {
      get { return Descriptor; }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public VoiceSelectionParams() {
      OnConstruction();
    }

    partial void OnConstruction();

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public VoiceSelectionParams(VoiceSelectionParams other) : this() {
      name_ = other.name_;
      ssmlGender_ = other.ssmlGender_;
      _unknownFields = pb::UnknownFieldSet.Clone(other._unknownFields);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public VoiceSelectionParams Clone() {
      return new VoiceSelectionParams(this);
    }

    /// <summary>Field number for the "name" field.</summary>
    public const int NameFieldNumber = 1;
    private string name_ = "";
    /// <summary>
    /// Optional. The name of the voice. If not set, the service will choose a
    /// voice based on the other parameters such as language_code and
    /// [ssml_gender][google.cloud.dialogflow.v2.VoiceSelectionParams.ssml_gender].
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public string Name {
      get { return name_; }
      set {
        name_ = pb::ProtoPreconditions.CheckNotNull(value, "value");
      }
    }

    /// <summary>Field number for the "ssml_gender" field.</summary>
    public const int SsmlGenderFieldNumber = 2;
    private global::Google.Cloud.Dialogflow.V2.SsmlVoiceGender ssmlGender_ = 0;
    /// <summary>
    /// Optional. The preferred gender of the voice. If not set, the service will
    /// choose a voice based on the other parameters such as language_code and
    /// [name][google.cloud.dialogflow.v2.VoiceSelectionParams.name]. Note that this is only a preference, not requirement. If a
    /// voice of the appropriate gender is not available, the synthesizer should
    /// substitute a voice with a different gender rather than failing the request.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public global::Google.Cloud.Dialogflow.V2.SsmlVoiceGender SsmlGender {
      get { return ssmlGender_; }
      set {
        ssmlGender_ = value;
      }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public override bool Equals(object other) {
      return Equals(other as VoiceSelectionParams);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public bool Equals(VoiceSelectionParams other) {
      if (ReferenceEquals(other, null)) {
        return false;
      }
      if (ReferenceEquals(other, this)) {
        return true;
      }
      if (Name != other.Name) return false;
      if (SsmlGender != other.SsmlGender) return false;
      return Equals(_unknownFields, other._unknownFields);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public override int GetHashCode() {
      int hash = 1;
      if (Name.Length != 0) hash ^= Name.GetHashCode();
      if (SsmlGender != 0) hash ^= SsmlGender.GetHashCode();
      if (_unknownFields != null) {
        hash ^= _unknownFields.GetHashCode();
      }
      return hash;
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public override string ToString() {
      return pb::JsonFormatter.ToDiagnosticString(this);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public void WriteTo(pb::CodedOutputStream output) {
      if (Name.Length != 0) {
        output.WriteRawTag(10);
        output.WriteString(Name);
      }
      if (SsmlGender != 0) {
        output.WriteRawTag(16);
        output.WriteEnum((int) SsmlGender);
      }
      if (_unknownFields != null) {
        _unknownFields.WriteTo(output);
      }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public int CalculateSize() {
      int size = 0;
      if (Name.Length != 0) {
        size += 1 + pb::CodedOutputStream.ComputeStringSize(Name);
      }
      if (SsmlGender != 0) {
        size += 1 + pb::CodedOutputStream.ComputeEnumSize((int) SsmlGender);
      }
      if (_unknownFields != null) {
        size += _unknownFields.CalculateSize();
      }
      return size;
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public void MergeFrom(VoiceSelectionParams other) {
      if (other == null) {
        return;
      }
      if (other.Name.Length != 0) {
        Name = other.Name;
      }
      if (other.SsmlGender != 0) {
        SsmlGender = other.SsmlGender;
      }
      _unknownFields = pb::UnknownFieldSet.MergeFrom(_unknownFields, other._unknownFields);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public void MergeFrom(pb::CodedInputStream input) {
      uint tag;
      while ((tag = input.ReadTag()) != 0) {
        switch(tag) {
          default:
            _unknownFields = pb::UnknownFieldSet.MergeFieldFrom(_unknownFields, input);
            break;
          case 10: {
            Name = input.ReadString();
            break;
          }
          case 16: {
            SsmlGender = (global::Google.Cloud.Dialogflow.V2.SsmlVoiceGender) input.ReadEnum();
            break;
          }
        }
      }
    }

  }

  /// <summary>
  /// Configuration of how speech should be synthesized.
  /// </summary>
  public sealed partial class SynthesizeSpeechConfig : pb::IMessage<SynthesizeSpeechConfig> {
    private static readonly pb::MessageParser<SynthesizeSpeechConfig> _parser = new pb::MessageParser<SynthesizeSpeechConfig>(() => new SynthesizeSpeechConfig());
    private pb::UnknownFieldSet _unknownFields;
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public static pb::MessageParser<SynthesizeSpeechConfig> Parser { get { return _parser; } }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public static pbr::MessageDescriptor Descriptor {
      get { return global::Google.Cloud.Dialogflow.V2.AudioConfigReflection.Descriptor.MessageTypes[2]; }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    pbr::MessageDescriptor pb::IMessage.Descriptor {
      get { return Descriptor; }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public SynthesizeSpeechConfig() {
      OnConstruction();
    }

    partial void OnConstruction();

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public SynthesizeSpeechConfig(SynthesizeSpeechConfig other) : this() {
      speakingRate_ = other.speakingRate_;
      pitch_ = other.pitch_;
      volumeGainDb_ = other.volumeGainDb_;
      effectsProfileId_ = other.effectsProfileId_.Clone();
      voice_ = other.voice_ != null ? other.voice_.Clone() : null;
      _unknownFields = pb::UnknownFieldSet.Clone(other._unknownFields);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public SynthesizeSpeechConfig Clone() {
      return new SynthesizeSpeechConfig(this);
    }

    /// <summary>Field number for the "speaking_rate" field.</summary>
    public const int SpeakingRateFieldNumber = 1;
    private double speakingRate_;
    /// <summary>
    /// Optional. Speaking rate/speed, in the range [0.25, 4.0]. 1.0 is the normal
    /// native speed supported by the specific voice. 2.0 is twice as fast, and
    /// 0.5 is half as fast. If unset(0.0), defaults to the native 1.0 speed. Any
    /// other values &lt; 0.25 or > 4.0 will return an error.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public double SpeakingRate {
      get { return speakingRate_; }
      set {
        speakingRate_ = value;
      }
    }

    /// <summary>Field number for the "pitch" field.</summary>
    public const int PitchFieldNumber = 2;
    private double pitch_;
    /// <summary>
    /// Optional. Speaking pitch, in the range [-20.0, 20.0]. 20 means increase 20
    /// semitones from the original pitch. -20 means decrease 20 semitones from the
    /// original pitch.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public double Pitch {
      get { return pitch_; }
      set {
        pitch_ = value;
      }
    }

    /// <summary>Field number for the "volume_gain_db" field.</summary>
    public const int VolumeGainDbFieldNumber = 3;
    private double volumeGainDb_;
    /// <summary>
    /// Optional. Volume gain (in dB) of the normal native volume supported by the
    /// specific voice, in the range [-96.0, 16.0]. If unset, or set to a value of
    /// 0.0 (dB), will play at normal native signal amplitude. A value of -6.0 (dB)
    /// will play at approximately half the amplitude of the normal native signal
    /// amplitude. A value of +6.0 (dB) will play at approximately twice the
    /// amplitude of the normal native signal amplitude. We strongly recommend not
    /// to exceed +10 (dB) as there's usually no effective increase in loudness for
    /// any value greater than that.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public double VolumeGainDb {
      get { return volumeGainDb_; }
      set {
        volumeGainDb_ = value;
      }
    }

    /// <summary>Field number for the "effects_profile_id" field.</summary>
    public const int EffectsProfileIdFieldNumber = 5;
    private static readonly pb::FieldCodec<string> _repeated_effectsProfileId_codec
        = pb::FieldCodec.ForString(42);
    private readonly pbc::RepeatedField<string> effectsProfileId_ = new pbc::RepeatedField<string>();
    /// <summary>
    /// Optional. An identifier which selects 'audio effects' profiles that are
    /// applied on (post synthesized) text to speech. Effects are applied on top of
    /// each other in the order they are given.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public pbc::RepeatedField<string> EffectsProfileId {
      get { return effectsProfileId_; }
    }

    /// <summary>Field number for the "voice" field.</summary>
    public const int VoiceFieldNumber = 4;
    private global::Google.Cloud.Dialogflow.V2.VoiceSelectionParams voice_;
    /// <summary>
    /// Optional. The desired voice of the synthesized audio.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public global::Google.Cloud.Dialogflow.V2.VoiceSelectionParams Voice {
      get { return voice_; }
      set {
        voice_ = value;
      }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public override bool Equals(object other) {
      return Equals(other as SynthesizeSpeechConfig);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public bool Equals(SynthesizeSpeechConfig other) {
      if (ReferenceEquals(other, null)) {
        return false;
      }
      if (ReferenceEquals(other, this)) {
        return true;
      }
      if (!pbc::ProtobufEqualityComparers.BitwiseDoubleEqualityComparer.Equals(SpeakingRate, other.SpeakingRate)) return false;
      if (!pbc::ProtobufEqualityComparers.BitwiseDoubleEqualityComparer.Equals(Pitch, other.Pitch)) return false;
      if (!pbc::ProtobufEqualityComparers.BitwiseDoubleEqualityComparer.Equals(VolumeGainDb, other.VolumeGainDb)) return false;
      if(!effectsProfileId_.Equals(other.effectsProfileId_)) return false;
      if (!object.Equals(Voice, other.Voice)) return false;
      return Equals(_unknownFields, other._unknownFields);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public override int GetHashCode() {
      int hash = 1;
      if (SpeakingRate != 0D) hash ^= pbc::ProtobufEqualityComparers.BitwiseDoubleEqualityComparer.GetHashCode(SpeakingRate);
      if (Pitch != 0D) hash ^= pbc::ProtobufEqualityComparers.BitwiseDoubleEqualityComparer.GetHashCode(Pitch);
      if (VolumeGainDb != 0D) hash ^= pbc::ProtobufEqualityComparers.BitwiseDoubleEqualityComparer.GetHashCode(VolumeGainDb);
      hash ^= effectsProfileId_.GetHashCode();
      if (voice_ != null) hash ^= Voice.GetHashCode();
      if (_unknownFields != null) {
        hash ^= _unknownFields.GetHashCode();
      }
      return hash;
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public override string ToString() {
      return pb::JsonFormatter.ToDiagnosticString(this);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public void WriteTo(pb::CodedOutputStream output) {
      if (SpeakingRate != 0D) {
        output.WriteRawTag(9);
        output.WriteDouble(SpeakingRate);
      }
      if (Pitch != 0D) {
        output.WriteRawTag(17);
        output.WriteDouble(Pitch);
      }
      if (VolumeGainDb != 0D) {
        output.WriteRawTag(25);
        output.WriteDouble(VolumeGainDb);
      }
      if (voice_ != null) {
        output.WriteRawTag(34);
        output.WriteMessage(Voice);
      }
      effectsProfileId_.WriteTo(output, _repeated_effectsProfileId_codec);
      if (_unknownFields != null) {
        _unknownFields.WriteTo(output);
      }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public int CalculateSize() {
      int size = 0;
      if (SpeakingRate != 0D) {
        size += 1 + 8;
      }
      if (Pitch != 0D) {
        size += 1 + 8;
      }
      if (VolumeGainDb != 0D) {
        size += 1 + 8;
      }
      size += effectsProfileId_.CalculateSize(_repeated_effectsProfileId_codec);
      if (voice_ != null) {
        size += 1 + pb::CodedOutputStream.ComputeMessageSize(Voice);
      }
      if (_unknownFields != null) {
        size += _unknownFields.CalculateSize();
      }
      return size;
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public void MergeFrom(SynthesizeSpeechConfig other) {
      if (other == null) {
        return;
      }
      if (other.SpeakingRate != 0D) {
        SpeakingRate = other.SpeakingRate;
      }
      if (other.Pitch != 0D) {
        Pitch = other.Pitch;
      }
      if (other.VolumeGainDb != 0D) {
        VolumeGainDb = other.VolumeGainDb;
      }
      effectsProfileId_.Add(other.effectsProfileId_);
      if (other.voice_ != null) {
        if (voice_ == null) {
          Voice = new global::Google.Cloud.Dialogflow.V2.VoiceSelectionParams();
        }
        Voice.MergeFrom(other.Voice);
      }
      _unknownFields = pb::UnknownFieldSet.MergeFrom(_unknownFields, other._unknownFields);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public void MergeFrom(pb::CodedInputStream input) {
      uint tag;
      while ((tag = input.ReadTag()) != 0) {
        switch(tag) {
          default:
            _unknownFields = pb::UnknownFieldSet.MergeFieldFrom(_unknownFields, input);
            break;
          case 9: {
            SpeakingRate = input.ReadDouble();
            break;
          }
          case 17: {
            Pitch = input.ReadDouble();
            break;
          }
          case 25: {
            VolumeGainDb = input.ReadDouble();
            break;
          }
          case 34: {
            if (voice_ == null) {
              Voice = new global::Google.Cloud.Dialogflow.V2.VoiceSelectionParams();
            }
            input.ReadMessage(Voice);
            break;
          }
          case 42: {
            effectsProfileId_.AddEntriesFrom(input, _repeated_effectsProfileId_codec);
            break;
          }
        }
      }
    }

  }

  /// <summary>
  /// Instructs the speech synthesizer on how to generate the output audio content.
  /// </summary>
  public sealed partial class OutputAudioConfig : pb::IMessage<OutputAudioConfig> {
    private static readonly pb::MessageParser<OutputAudioConfig> _parser = new pb::MessageParser<OutputAudioConfig>(() => new OutputAudioConfig());
    private pb::UnknownFieldSet _unknownFields;
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public static pb::MessageParser<OutputAudioConfig> Parser { get { return _parser; } }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public static pbr::MessageDescriptor Descriptor {
      get { return global::Google.Cloud.Dialogflow.V2.AudioConfigReflection.Descriptor.MessageTypes[3]; }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    pbr::MessageDescriptor pb::IMessage.Descriptor {
      get { return Descriptor; }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public OutputAudioConfig() {
      OnConstruction();
    }

    partial void OnConstruction();

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public OutputAudioConfig(OutputAudioConfig other) : this() {
      audioEncoding_ = other.audioEncoding_;
      sampleRateHertz_ = other.sampleRateHertz_;
      synthesizeSpeechConfig_ = other.synthesizeSpeechConfig_ != null ? other.synthesizeSpeechConfig_.Clone() : null;
      _unknownFields = pb::UnknownFieldSet.Clone(other._unknownFields);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public OutputAudioConfig Clone() {
      return new OutputAudioConfig(this);
    }

    /// <summary>Field number for the "audio_encoding" field.</summary>
    public const int AudioEncodingFieldNumber = 1;
    private global::Google.Cloud.Dialogflow.V2.OutputAudioEncoding audioEncoding_ = 0;
    /// <summary>
    /// Required. Audio encoding of the synthesized audio content.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public global::Google.Cloud.Dialogflow.V2.OutputAudioEncoding AudioEncoding {
      get { return audioEncoding_; }
      set {
        audioEncoding_ = value;
      }
    }

    /// <summary>Field number for the "sample_rate_hertz" field.</summary>
    public const int SampleRateHertzFieldNumber = 2;
    private int sampleRateHertz_;
    /// <summary>
    /// Optional. The synthesis sample rate (in hertz) for this audio. If not
    /// provided, then the synthesizer will use the default sample rate based on
    /// the audio encoding. If this is different from the voice's natural sample
    /// rate, then the synthesizer will honor this request by converting to the
    /// desired sample rate (which might result in worse audio quality).
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public int SampleRateHertz {
      get { return sampleRateHertz_; }
      set {
        sampleRateHertz_ = value;
      }
    }

    /// <summary>Field number for the "synthesize_speech_config" field.</summary>
    public const int SynthesizeSpeechConfigFieldNumber = 3;
    private global::Google.Cloud.Dialogflow.V2.SynthesizeSpeechConfig synthesizeSpeechConfig_;
    /// <summary>
    /// Optional. Configuration of how speech should be synthesized.
    /// </summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public global::Google.Cloud.Dialogflow.V2.SynthesizeSpeechConfig SynthesizeSpeechConfig {
      get { return synthesizeSpeechConfig_; }
      set {
        synthesizeSpeechConfig_ = value;
      }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public override bool Equals(object other) {
      return Equals(other as OutputAudioConfig);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public bool Equals(OutputAudioConfig other) {
      if (ReferenceEquals(other, null)) {
        return false;
      }
      if (ReferenceEquals(other, this)) {
        return true;
      }
      if (AudioEncoding != other.AudioEncoding) return false;
      if (SampleRateHertz != other.SampleRateHertz) return false;
      if (!object.Equals(SynthesizeSpeechConfig, other.SynthesizeSpeechConfig)) return false;
      return Equals(_unknownFields, other._unknownFields);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public override int GetHashCode() {
      int hash = 1;
      if (AudioEncoding != 0) hash ^= AudioEncoding.GetHashCode();
      if (SampleRateHertz != 0) hash ^= SampleRateHertz.GetHashCode();
      if (synthesizeSpeechConfig_ != null) hash ^= SynthesizeSpeechConfig.GetHashCode();
      if (_unknownFields != null) {
        hash ^= _unknownFields.GetHashCode();
      }
      return hash;
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public override string ToString() {
      return pb::JsonFormatter.ToDiagnosticString(this);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public void WriteTo(pb::CodedOutputStream output) {
      if (AudioEncoding != 0) {
        output.WriteRawTag(8);
        output.WriteEnum((int) AudioEncoding);
      }
      if (SampleRateHertz != 0) {
        output.WriteRawTag(16);
        output.WriteInt32(SampleRateHertz);
      }
      if (synthesizeSpeechConfig_ != null) {
        output.WriteRawTag(26);
        output.WriteMessage(SynthesizeSpeechConfig);
      }
      if (_unknownFields != null) {
        _unknownFields.WriteTo(output);
      }
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public int CalculateSize() {
      int size = 0;
      if (AudioEncoding != 0) {
        size += 1 + pb::CodedOutputStream.ComputeEnumSize((int) AudioEncoding);
      }
      if (SampleRateHertz != 0) {
        size += 1 + pb::CodedOutputStream.ComputeInt32Size(SampleRateHertz);
      }
      if (synthesizeSpeechConfig_ != null) {
        size += 1 + pb::CodedOutputStream.ComputeMessageSize(SynthesizeSpeechConfig);
      }
      if (_unknownFields != null) {
        size += _unknownFields.CalculateSize();
      }
      return size;
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public void MergeFrom(OutputAudioConfig other) {
      if (other == null) {
        return;
      }
      if (other.AudioEncoding != 0) {
        AudioEncoding = other.AudioEncoding;
      }
      if (other.SampleRateHertz != 0) {
        SampleRateHertz = other.SampleRateHertz;
      }
      if (other.synthesizeSpeechConfig_ != null) {
        if (synthesizeSpeechConfig_ == null) {
          SynthesizeSpeechConfig = new global::Google.Cloud.Dialogflow.V2.SynthesizeSpeechConfig();
        }
        SynthesizeSpeechConfig.MergeFrom(other.SynthesizeSpeechConfig);
      }
      _unknownFields = pb::UnknownFieldSet.MergeFrom(_unknownFields, other._unknownFields);
    }

    [global::System.Diagnostics.DebuggerNonUserCodeAttribute]
    public void MergeFrom(pb::CodedInputStream input) {
      uint tag;
      while ((tag = input.ReadTag()) != 0) {
        switch(tag) {
          default:
            _unknownFields = pb::UnknownFieldSet.MergeFieldFrom(_unknownFields, input);
            break;
          case 8: {
            AudioEncoding = (global::Google.Cloud.Dialogflow.V2.OutputAudioEncoding) input.ReadEnum();
            break;
          }
          case 16: {
            SampleRateHertz = input.ReadInt32();
            break;
          }
          case 26: {
            if (synthesizeSpeechConfig_ == null) {
              SynthesizeSpeechConfig = new global::Google.Cloud.Dialogflow.V2.SynthesizeSpeechConfig();
            }
            input.ReadMessage(SynthesizeSpeechConfig);
            break;
          }
        }
      }
    }

  }

  #endregion

}

#endregion Designer generated code
