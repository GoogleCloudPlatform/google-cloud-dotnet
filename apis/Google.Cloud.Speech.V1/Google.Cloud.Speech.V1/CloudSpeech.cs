// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/cloud/speech/v1/cloud_speech.proto
#pragma warning disable 1591, 0612, 3021
#region Designer generated code

using pb = global::Google.Protobuf;
using pbc = global::Google.Protobuf.Collections;
using pbr = global::Google.Protobuf.Reflection;
using scg = global::System.Collections.Generic;
namespace Google.Cloud.Speech.V1 {

  /// <summary>Holder for reflection information generated from google/cloud/speech/v1/cloud_speech.proto</summary>
  [global::System.Diagnostics.DebuggerNonUserCodeAttribute()]
  public static partial class CloudSpeechReflection {

    #region Descriptor
    /// <summary>File descriptor for google/cloud/speech/v1/cloud_speech.proto</summary>
    public static pbr::FileDescriptor Descriptor {
      get { return descriptor; }
    }
    private static pbr::FileDescriptor descriptor;

    static CloudSpeechReflection() {
      byte[] descriptorData = global::System.Convert.FromBase64String(
          string.Concat(
            "Cilnb29nbGUvY2xvdWQvc3BlZWNoL3YxL2Nsb3VkX3NwZWVjaC5wcm90bxIW",
            "Z29vZ2xlLmNsb3VkLnNwZWVjaC52MRocZ29vZ2xlL2FwaS9hbm5vdGF0aW9u",
            "cy5wcm90bxoXZ29vZ2xlL3JwYy9zdGF0dXMucHJvdG8imQEKEFJlY29nbml6",
            "ZVJlcXVlc3QSSAoPaW5pdGlhbF9yZXF1ZXN0GAEgASgLMi8uZ29vZ2xlLmNs",
            "b3VkLnNwZWVjaC52MS5Jbml0aWFsUmVjb2duaXplUmVxdWVzdBI7Cg1hdWRp",
            "b19yZXF1ZXN0GAIgASgLMiQuZ29vZ2xlLmNsb3VkLnNwZWVjaC52MS5BdWRp",
            "b1JlcXVlc3QizwMKF0luaXRpYWxSZWNvZ25pemVSZXF1ZXN0Ek8KCGVuY29k",
            "aW5nGAEgASgOMj0uZ29vZ2xlLmNsb3VkLnNwZWVjaC52MS5Jbml0aWFsUmVj",
            "b2duaXplUmVxdWVzdC5BdWRpb0VuY29kaW5nEhMKC3NhbXBsZV9yYXRlGAIg",
            "ASgFEhUKDWxhbmd1YWdlX2NvZGUYAyABKAkSGAoQbWF4X2FsdGVybmF0aXZl",
            "cxgEIAEoBRIYChBwcm9mYW5pdHlfZmlsdGVyGAUgASgIEhIKCmNvbnRpbnVv",
            "dXMYBiABKAgSFwoPaW50ZXJpbV9yZXN1bHRzGAcgASgIEiAKGGVuYWJsZV9l",
            "bmRwb2ludGVyX2V2ZW50cxgIIAEoCBISCgpvdXRwdXRfdXJpGAkgASgJEj0K",
            "DnNwZWVjaF9jb250ZXh0GAogASgLMiUuZ29vZ2xlLmNsb3VkLnNwZWVjaC52",
            "MS5TcGVlY2hDb250ZXh0ImEKDUF1ZGlvRW5jb2RpbmcSGAoURU5DT0RJTkdf",
            "VU5TUEVDSUZJRUQQABIMCghMSU5FQVIxNhABEggKBEZMQUMQAhIJCgVNVUxB",
            "VxADEgcKA0FNUhAEEgoKBkFNUl9XQhAFIiAKDVNwZWVjaENvbnRleHQSDwoH",
            "cGhyYXNlcxgBIAMoCSIsCgxBdWRpb1JlcXVlc3QSDwoHY29udGVudBgBIAEo",
            "DBILCgN1cmkYAiABKAkiXQodTm9uU3RyZWFtaW5nUmVjb2duaXplUmVzcG9u",
            "c2USPAoJcmVzcG9uc2VzGAEgAygLMikuZ29vZ2xlLmNsb3VkLnNwZWVjaC52",
            "MS5SZWNvZ25pemVSZXNwb25zZSLhAgoRUmVjb2duaXplUmVzcG9uc2USIQoF",
            "ZXJyb3IYASABKAsyEi5nb29nbGUucnBjLlN0YXR1cxJACgdyZXN1bHRzGAIg",
            "AygLMi8uZ29vZ2xlLmNsb3VkLnNwZWVjaC52MS5TcGVlY2hSZWNvZ25pdGlv",
            "blJlc3VsdBIUCgxyZXN1bHRfaW5kZXgYAyABKAUSSwoIZW5kcG9pbnQYBCAB",
            "KA4yOS5nb29nbGUuY2xvdWQuc3BlZWNoLnYxLlJlY29nbml6ZVJlc3BvbnNl",
            "LkVuZHBvaW50ZXJFdmVudCKDAQoPRW5kcG9pbnRlckV2ZW50EiAKHEVORFBP",
            "SU5URVJfRVZFTlRfVU5TUEVDSUZJRUQQABITCg9TVEFSVF9PRl9TUEVFQ0gQ",
            "ARIRCg1FTkRfT0ZfU1BFRUNIEAISEAoMRU5EX09GX0FVRElPEAMSFAoQRU5E",
            "X09GX1VUVEVSQU5DRRAEIooBChdTcGVlY2hSZWNvZ25pdGlvblJlc3VsdBJK",
            "CgxhbHRlcm5hdGl2ZXMYASADKAsyNC5nb29nbGUuY2xvdWQuc3BlZWNoLnYx",
            "LlNwZWVjaFJlY29nbml0aW9uQWx0ZXJuYXRpdmUSEAoIaXNfZmluYWwYAiAB",
            "KAgSEQoJc3RhYmlsaXR5GAMgASgCIkYKHFNwZWVjaFJlY29nbml0aW9uQWx0",
            "ZXJuYXRpdmUSEgoKdHJhbnNjcmlwdBgBIAEoCRISCgpjb25maWRlbmNlGAIg",
            "ASgCMooCCgZTcGVlY2gSZAoJUmVjb2duaXplEiguZ29vZ2xlLmNsb3VkLnNw",
            "ZWVjaC52MS5SZWNvZ25pemVSZXF1ZXN0GikuZ29vZ2xlLmNsb3VkLnNwZWVj",
            "aC52MS5SZWNvZ25pemVSZXNwb25zZSgBMAESmQEKFU5vblN0cmVhbWluZ1Jl",
            "Y29nbml6ZRIoLmdvb2dsZS5jbG91ZC5zcGVlY2gudjEuUmVjb2duaXplUmVx",
            "dWVzdBo1Lmdvb2dsZS5jbG91ZC5zcGVlY2gudjEuTm9uU3RyZWFtaW5nUmVj",
            "b2duaXplUmVzcG9uc2UiH4LT5JMCGSIUL3YxL3NwZWVjaDpyZWNvZ25pemU6",
            "ASpCYgoaY29tLmdvb2dsZS5jbG91ZC5zcGVlY2gudjFCC1NwZWVjaFByb3Rv",
            "UAFaNWdvb2dsZS5nb2xhbmcub3JnL2dlbnByb3RvL2dvb2dsZWFwaXMvY2xv",
            "dWQvc3BlZWNoL3YxYgZwcm90bzM="));
      descriptor = pbr::FileDescriptor.FromGeneratedCode(descriptorData,
          new pbr::FileDescriptor[] { global::Google.Api.AnnotationsReflection.Descriptor, global::Google.Rpc.StatusReflection.Descriptor, },
          new pbr::GeneratedClrTypeInfo(null, new pbr::GeneratedClrTypeInfo[] {
            new pbr::GeneratedClrTypeInfo(typeof(global::Google.Cloud.Speech.V1.RecognizeRequest), global::Google.Cloud.Speech.V1.RecognizeRequest.Parser, new[]{ "InitialRequest", "AudioRequest" }, null, null, null),
            new pbr::GeneratedClrTypeInfo(typeof(global::Google.Cloud.Speech.V1.InitialRecognizeRequest), global::Google.Cloud.Speech.V1.InitialRecognizeRequest.Parser, new[]{ "Encoding", "SampleRate", "LanguageCode", "MaxAlternatives", "ProfanityFilter", "Continuous", "InterimResults", "EnableEndpointerEvents", "OutputUri", "SpeechContext" }, null, new[]{ typeof(global::Google.Cloud.Speech.V1.InitialRecognizeRequest.Types.AudioEncoding) }, null),
            new pbr::GeneratedClrTypeInfo(typeof(global::Google.Cloud.Speech.V1.SpeechContext), global::Google.Cloud.Speech.V1.SpeechContext.Parser, new[]{ "Phrases" }, null, null, null),
            new pbr::GeneratedClrTypeInfo(typeof(global::Google.Cloud.Speech.V1.AudioRequest), global::Google.Cloud.Speech.V1.AudioRequest.Parser, new[]{ "Content", "Uri" }, null, null, null),
            new pbr::GeneratedClrTypeInfo(typeof(global::Google.Cloud.Speech.V1.NonStreamingRecognizeResponse), global::Google.Cloud.Speech.V1.NonStreamingRecognizeResponse.Parser, new[]{ "Responses" }, null, null, null),
            new pbr::GeneratedClrTypeInfo(typeof(global::Google.Cloud.Speech.V1.RecognizeResponse), global::Google.Cloud.Speech.V1.RecognizeResponse.Parser, new[]{ "Error", "Results", "ResultIndex", "Endpoint" }, null, new[]{ typeof(global::Google.Cloud.Speech.V1.RecognizeResponse.Types.EndpointerEvent) }, null),
            new pbr::GeneratedClrTypeInfo(typeof(global::Google.Cloud.Speech.V1.SpeechRecognitionResult), global::Google.Cloud.Speech.V1.SpeechRecognitionResult.Parser, new[]{ "Alternatives", "IsFinal", "Stability" }, null, null, null),
            new pbr::GeneratedClrTypeInfo(typeof(global::Google.Cloud.Speech.V1.SpeechRecognitionAlternative), global::Google.Cloud.Speech.V1.SpeechRecognitionAlternative.Parser, new[]{ "Transcript", "Confidence" }, null, null, null)
          }));
    }
    #endregion

  }
  #region Messages
  /// <summary>
  ///  `RecognizeRequest` is the only message type sent by the client.
  ///
  ///  When using the REST API or the gRPC `NonStreamingRecognize` API, only one
  ///  `RecognizeRequest` message is sent, and it must contain both an
  ///  `initial_request` and an 'audio_request`.
  ///
  ///  When using the gRPC Streaming `Recognize` API, one or more `RecognizeRequest`
  ///  messages are sent. The first message must contain an `initial_request` and
  ///  may contain an 'audio_request`. Any subsequent messages must not contain an
  ///  `initial_request` and must contain an 'audio_request`.
  /// </summary>
  [global::System.Diagnostics.DebuggerNonUserCodeAttribute()]
  public sealed partial class RecognizeRequest : pb::IMessage<RecognizeRequest> {
    private static readonly pb::MessageParser<RecognizeRequest> _parser = new pb::MessageParser<RecognizeRequest>(() => new RecognizeRequest());
    public static pb::MessageParser<RecognizeRequest> Parser { get { return _parser; } }

    public static pbr::MessageDescriptor Descriptor {
      get { return global::Google.Cloud.Speech.V1.CloudSpeechReflection.Descriptor.MessageTypes[0]; }
    }

    pbr::MessageDescriptor pb::IMessage.Descriptor {
      get { return Descriptor; }
    }

    public RecognizeRequest() {
      OnConstruction();
    }

    partial void OnConstruction();

    public RecognizeRequest(RecognizeRequest other) : this() {
      InitialRequest = other.initialRequest_ != null ? other.InitialRequest.Clone() : null;
      AudioRequest = other.audioRequest_ != null ? other.AudioRequest.Clone() : null;
    }

    public RecognizeRequest Clone() {
      return new RecognizeRequest(this);
    }

    /// <summary>Field number for the "initial_request" field.</summary>
    public const int InitialRequestFieldNumber = 1;
    private global::Google.Cloud.Speech.V1.InitialRecognizeRequest initialRequest_;
    /// <summary>
    ///  The `initial_request` message provides information to the recognizer
    ///  that specifies how to process the request.
    ///
    ///  The first `RecognizeRequest` message must contain an `initial_request`.
    ///  Any subsequent `RecognizeRequest` messages must not contain an
    ///  `initial_request`.
    /// </summary>
    public global::Google.Cloud.Speech.V1.InitialRecognizeRequest InitialRequest {
      get { return initialRequest_; }
      set {
        initialRequest_ = value;
      }
    }

    /// <summary>Field number for the "audio_request" field.</summary>
    public const int AudioRequestFieldNumber = 2;
    private global::Google.Cloud.Speech.V1.AudioRequest audioRequest_;
    /// <summary>
    ///  The audio data to be recognized. For REST or `NonStreamingRecognize`, all
    ///  audio data must be contained in the first (and only) `RecognizeRequest`
    ///  message. For gRPC streaming `Recognize`, sequential chunks of audio data
    ///  are sent in sequential `RecognizeRequest` messages.
    /// </summary>
    public global::Google.Cloud.Speech.V1.AudioRequest AudioRequest {
      get { return audioRequest_; }
      set {
        audioRequest_ = value;
      }
    }

    public override bool Equals(object other) {
      return Equals(other as RecognizeRequest);
    }

    public bool Equals(RecognizeRequest other) {
      if (ReferenceEquals(other, null)) {
        return false;
      }
      if (ReferenceEquals(other, this)) {
        return true;
      }
      if (!object.Equals(InitialRequest, other.InitialRequest)) return false;
      if (!object.Equals(AudioRequest, other.AudioRequest)) return false;
      return true;
    }

    public override int GetHashCode() {
      int hash = 1;
      if (initialRequest_ != null) hash ^= InitialRequest.GetHashCode();
      if (audioRequest_ != null) hash ^= AudioRequest.GetHashCode();
      return hash;
    }

    public override string ToString() {
      return pb::JsonFormatter.ToDiagnosticString(this);
    }

    public void WriteTo(pb::CodedOutputStream output) {
      if (initialRequest_ != null) {
        output.WriteRawTag(10);
        output.WriteMessage(InitialRequest);
      }
      if (audioRequest_ != null) {
        output.WriteRawTag(18);
        output.WriteMessage(AudioRequest);
      }
    }

    public int CalculateSize() {
      int size = 0;
      if (initialRequest_ != null) {
        size += 1 + pb::CodedOutputStream.ComputeMessageSize(InitialRequest);
      }
      if (audioRequest_ != null) {
        size += 1 + pb::CodedOutputStream.ComputeMessageSize(AudioRequest);
      }
      return size;
    }

    public void MergeFrom(RecognizeRequest other) {
      if (other == null) {
        return;
      }
      if (other.initialRequest_ != null) {
        if (initialRequest_ == null) {
          initialRequest_ = new global::Google.Cloud.Speech.V1.InitialRecognizeRequest();
        }
        InitialRequest.MergeFrom(other.InitialRequest);
      }
      if (other.audioRequest_ != null) {
        if (audioRequest_ == null) {
          audioRequest_ = new global::Google.Cloud.Speech.V1.AudioRequest();
        }
        AudioRequest.MergeFrom(other.AudioRequest);
      }
    }

    public void MergeFrom(pb::CodedInputStream input) {
      uint tag;
      while ((tag = input.ReadTag()) != 0) {
        switch(tag) {
          default:
            input.SkipLastField();
            break;
          case 10: {
            if (initialRequest_ == null) {
              initialRequest_ = new global::Google.Cloud.Speech.V1.InitialRecognizeRequest();
            }
            input.ReadMessage(initialRequest_);
            break;
          }
          case 18: {
            if (audioRequest_ == null) {
              audioRequest_ = new global::Google.Cloud.Speech.V1.AudioRequest();
            }
            input.ReadMessage(audioRequest_);
            break;
          }
        }
      }
    }

  }

  /// <summary>
  ///  The `InitialRecognizeRequest` message provides information to the recognizer
  ///  that specifies how to process the request.
  /// </summary>
  [global::System.Diagnostics.DebuggerNonUserCodeAttribute()]
  public sealed partial class InitialRecognizeRequest : pb::IMessage<InitialRecognizeRequest> {
    private static readonly pb::MessageParser<InitialRecognizeRequest> _parser = new pb::MessageParser<InitialRecognizeRequest>(() => new InitialRecognizeRequest());
    public static pb::MessageParser<InitialRecognizeRequest> Parser { get { return _parser; } }

    public static pbr::MessageDescriptor Descriptor {
      get { return global::Google.Cloud.Speech.V1.CloudSpeechReflection.Descriptor.MessageTypes[1]; }
    }

    pbr::MessageDescriptor pb::IMessage.Descriptor {
      get { return Descriptor; }
    }

    public InitialRecognizeRequest() {
      OnConstruction();
    }

    partial void OnConstruction();

    public InitialRecognizeRequest(InitialRecognizeRequest other) : this() {
      encoding_ = other.encoding_;
      sampleRate_ = other.sampleRate_;
      languageCode_ = other.languageCode_;
      maxAlternatives_ = other.maxAlternatives_;
      profanityFilter_ = other.profanityFilter_;
      continuous_ = other.continuous_;
      interimResults_ = other.interimResults_;
      enableEndpointerEvents_ = other.enableEndpointerEvents_;
      outputUri_ = other.outputUri_;
      SpeechContext = other.speechContext_ != null ? other.SpeechContext.Clone() : null;
    }

    public InitialRecognizeRequest Clone() {
      return new InitialRecognizeRequest(this);
    }

    /// <summary>Field number for the "encoding" field.</summary>
    public const int EncodingFieldNumber = 1;
    private global::Google.Cloud.Speech.V1.InitialRecognizeRequest.Types.AudioEncoding encoding_ = 0;
    /// <summary>
    ///  [Required] Encoding of audio data sent in all `AudioRequest` messages.
    /// </summary>
    public global::Google.Cloud.Speech.V1.InitialRecognizeRequest.Types.AudioEncoding Encoding {
      get { return encoding_; }
      set {
        encoding_ = value;
      }
    }

    /// <summary>Field number for the "sample_rate" field.</summary>
    public const int SampleRateFieldNumber = 2;
    private int sampleRate_;
    /// <summary>
    ///  [Required] Sample rate in Hertz of the audio data sent in all
    ///  AudioRequest messages. Valid values are: 8000-48000.
    ///  16000 is optimal. For best results, set the sampling rate of the audio
    ///  source to 16000 Hz. If that's not possible, use the native sample rate of
    ///  the audio source (instead of re-sampling).
    /// </summary>
    public int SampleRate {
      get { return sampleRate_; }
      set {
        sampleRate_ = value;
      }
    }

    /// <summary>Field number for the "language_code" field.</summary>
    public const int LanguageCodeFieldNumber = 3;
    private string languageCode_ = "";
    /// <summary>
    ///  [Optional] The language of the supplied audio as a BCP-47 language tag.
    ///  Example: "en-GB"  https://www.rfc-editor.org/rfc/bcp/bcp47.txt
    ///  If omitted, defaults to "en-US". See
    ///  [Language Support](/speech/docs/best-practices#language_support) for
    ///  a list of the currently supported language codes.
    /// </summary>
    public string LanguageCode {
      get { return languageCode_; }
      set {
        languageCode_ = pb::ProtoPreconditions.CheckNotNull(value, "value");
      }
    }

    /// <summary>Field number for the "max_alternatives" field.</summary>
    public const int MaxAlternativesFieldNumber = 4;
    private int maxAlternatives_;
    /// <summary>
    ///  [Optional] Maximum number of recognition hypotheses to be returned.
    ///  Specifically, the maximum number of `SpeechRecognitionAlternative` messages
    ///  within each `SpeechRecognitionResult`.
    ///  The server may return fewer than `max_alternatives`.
    ///  Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
    ///  `1`. If omitted, defaults to `1`.
    /// </summary>
    public int MaxAlternatives {
      get { return maxAlternatives_; }
      set {
        maxAlternatives_ = value;
      }
    }

    /// <summary>Field number for the "profanity_filter" field.</summary>
    public const int ProfanityFilterFieldNumber = 5;
    private bool profanityFilter_;
    /// <summary>
    ///  [Optional] If set to `true`, the server will attempt to filter out
    ///  profanities, replacing all but the initial character in each filtered word
    ///  with asterisks, e.g. "f***". If set to `false` or omitted, profanities
    ///  won't be filtered out.
    /// </summary>
    public bool ProfanityFilter {
      get { return profanityFilter_; }
      set {
        profanityFilter_ = value;
      }
    }

    /// <summary>Field number for the "continuous" field.</summary>
    public const int ContinuousFieldNumber = 6;
    private bool continuous_;
    /// <summary>
    ///  [Optional] If `false` or omitted, the recognizer will detect a single
    ///  spoken utterance, and it will cease recognition when the user stops
    ///  speaking. If `enable_endpointer_events` is `true`, it will return
    ///  `END_OF_UTTERANCE` when it detects that the user has stopped speaking.
    ///  In all cases, it will return no more than one `SpeechRecognitionResult`,
    ///  and set the `is_final` flag to `true`.
    ///
    ///  If `true`, the recognizer will continue recognition (even if the user
    ///  pauses speaking) until the client closes the output stream (gRPC API) or
    ///  completes the POST data (REST API) or when the maximum time limit has been
    ///  reached. Multiple `SpeechRecognitionResult`s with the `is_final` flag set
    ///  to `true` may be returned to indicate that the recognizer will not return
    ///  any further hypotheses for that portion of the transcript.
    /// </summary>
    public bool Continuous {
      get { return continuous_; }
      set {
        continuous_ = value;
      }
    }

    /// <summary>Field number for the "interim_results" field.</summary>
    public const int InterimResultsFieldNumber = 7;
    private bool interimResults_;
    /// <summary>
    ///  [Optional] If this parameter is `true`, interim results may be returned as
    ///  they become available.
    ///  If `false` or omitted, only `is_final=true` result(s) are returned.
    /// </summary>
    public bool InterimResults {
      get { return interimResults_; }
      set {
        interimResults_ = value;
      }
    }

    /// <summary>Field number for the "enable_endpointer_events" field.</summary>
    public const int EnableEndpointerEventsFieldNumber = 8;
    private bool enableEndpointerEvents_;
    /// <summary>
    ///  [Optional] If this parameter is `true`, `EndpointerEvents` may be returned
    ///  as they become available.
    ///  If `false` or omitted, no `EndpointerEvents` are returned.
    /// </summary>
    public bool EnableEndpointerEvents {
      get { return enableEndpointerEvents_; }
      set {
        enableEndpointerEvents_ = value;
      }
    }

    /// <summary>Field number for the "output_uri" field.</summary>
    public const int OutputUriFieldNumber = 9;
    private string outputUri_ = "";
    /// <summary>
    ///  [Optional] URI that points to a file where the recognition result should
    ///  be stored in JSON format. If omitted or empty string, the recognition
    ///  result is returned in the response. Should be specified only for
    ///  `NonStreamingRecognize`. If specified in a `Recognize` request,
    ///  `Recognize` returns [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
    ///  If specified in a `NonStreamingRecognize` request,
    ///  `NonStreamingRecognize` returns immediately, and the output file
    ///  is created asynchronously once the audio processing completes.
    ///  Currently, only Google Cloud Storage URIs are supported, which must be
    ///  specified in the following format: `gs://bucket_name/object_name`
    ///  (other URI formats return [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
    ///  more information, see [Request URIs](/storage/docs/reference-uris).
    /// </summary>
    public string OutputUri {
      get { return outputUri_; }
      set {
        outputUri_ = pb::ProtoPreconditions.CheckNotNull(value, "value");
      }
    }

    /// <summary>Field number for the "speech_context" field.</summary>
    public const int SpeechContextFieldNumber = 10;
    private global::Google.Cloud.Speech.V1.SpeechContext speechContext_;
    /// <summary>
    ///  [Optional] A means to provide context to assist the speech recognition.
    /// </summary>
    public global::Google.Cloud.Speech.V1.SpeechContext SpeechContext {
      get { return speechContext_; }
      set {
        speechContext_ = value;
      }
    }

    public override bool Equals(object other) {
      return Equals(other as InitialRecognizeRequest);
    }

    public bool Equals(InitialRecognizeRequest other) {
      if (ReferenceEquals(other, null)) {
        return false;
      }
      if (ReferenceEquals(other, this)) {
        return true;
      }
      if (Encoding != other.Encoding) return false;
      if (SampleRate != other.SampleRate) return false;
      if (LanguageCode != other.LanguageCode) return false;
      if (MaxAlternatives != other.MaxAlternatives) return false;
      if (ProfanityFilter != other.ProfanityFilter) return false;
      if (Continuous != other.Continuous) return false;
      if (InterimResults != other.InterimResults) return false;
      if (EnableEndpointerEvents != other.EnableEndpointerEvents) return false;
      if (OutputUri != other.OutputUri) return false;
      if (!object.Equals(SpeechContext, other.SpeechContext)) return false;
      return true;
    }

    public override int GetHashCode() {
      int hash = 1;
      if (Encoding != 0) hash ^= Encoding.GetHashCode();
      if (SampleRate != 0) hash ^= SampleRate.GetHashCode();
      if (LanguageCode.Length != 0) hash ^= LanguageCode.GetHashCode();
      if (MaxAlternatives != 0) hash ^= MaxAlternatives.GetHashCode();
      if (ProfanityFilter != false) hash ^= ProfanityFilter.GetHashCode();
      if (Continuous != false) hash ^= Continuous.GetHashCode();
      if (InterimResults != false) hash ^= InterimResults.GetHashCode();
      if (EnableEndpointerEvents != false) hash ^= EnableEndpointerEvents.GetHashCode();
      if (OutputUri.Length != 0) hash ^= OutputUri.GetHashCode();
      if (speechContext_ != null) hash ^= SpeechContext.GetHashCode();
      return hash;
    }

    public override string ToString() {
      return pb::JsonFormatter.ToDiagnosticString(this);
    }

    public void WriteTo(pb::CodedOutputStream output) {
      if (Encoding != 0) {
        output.WriteRawTag(8);
        output.WriteEnum((int) Encoding);
      }
      if (SampleRate != 0) {
        output.WriteRawTag(16);
        output.WriteInt32(SampleRate);
      }
      if (LanguageCode.Length != 0) {
        output.WriteRawTag(26);
        output.WriteString(LanguageCode);
      }
      if (MaxAlternatives != 0) {
        output.WriteRawTag(32);
        output.WriteInt32(MaxAlternatives);
      }
      if (ProfanityFilter != false) {
        output.WriteRawTag(40);
        output.WriteBool(ProfanityFilter);
      }
      if (Continuous != false) {
        output.WriteRawTag(48);
        output.WriteBool(Continuous);
      }
      if (InterimResults != false) {
        output.WriteRawTag(56);
        output.WriteBool(InterimResults);
      }
      if (EnableEndpointerEvents != false) {
        output.WriteRawTag(64);
        output.WriteBool(EnableEndpointerEvents);
      }
      if (OutputUri.Length != 0) {
        output.WriteRawTag(74);
        output.WriteString(OutputUri);
      }
      if (speechContext_ != null) {
        output.WriteRawTag(82);
        output.WriteMessage(SpeechContext);
      }
    }

    public int CalculateSize() {
      int size = 0;
      if (Encoding != 0) {
        size += 1 + pb::CodedOutputStream.ComputeEnumSize((int) Encoding);
      }
      if (SampleRate != 0) {
        size += 1 + pb::CodedOutputStream.ComputeInt32Size(SampleRate);
      }
      if (LanguageCode.Length != 0) {
        size += 1 + pb::CodedOutputStream.ComputeStringSize(LanguageCode);
      }
      if (MaxAlternatives != 0) {
        size += 1 + pb::CodedOutputStream.ComputeInt32Size(MaxAlternatives);
      }
      if (ProfanityFilter != false) {
        size += 1 + 1;
      }
      if (Continuous != false) {
        size += 1 + 1;
      }
      if (InterimResults != false) {
        size += 1 + 1;
      }
      if (EnableEndpointerEvents != false) {
        size += 1 + 1;
      }
      if (OutputUri.Length != 0) {
        size += 1 + pb::CodedOutputStream.ComputeStringSize(OutputUri);
      }
      if (speechContext_ != null) {
        size += 1 + pb::CodedOutputStream.ComputeMessageSize(SpeechContext);
      }
      return size;
    }

    public void MergeFrom(InitialRecognizeRequest other) {
      if (other == null) {
        return;
      }
      if (other.Encoding != 0) {
        Encoding = other.Encoding;
      }
      if (other.SampleRate != 0) {
        SampleRate = other.SampleRate;
      }
      if (other.LanguageCode.Length != 0) {
        LanguageCode = other.LanguageCode;
      }
      if (other.MaxAlternatives != 0) {
        MaxAlternatives = other.MaxAlternatives;
      }
      if (other.ProfanityFilter != false) {
        ProfanityFilter = other.ProfanityFilter;
      }
      if (other.Continuous != false) {
        Continuous = other.Continuous;
      }
      if (other.InterimResults != false) {
        InterimResults = other.InterimResults;
      }
      if (other.EnableEndpointerEvents != false) {
        EnableEndpointerEvents = other.EnableEndpointerEvents;
      }
      if (other.OutputUri.Length != 0) {
        OutputUri = other.OutputUri;
      }
      if (other.speechContext_ != null) {
        if (speechContext_ == null) {
          speechContext_ = new global::Google.Cloud.Speech.V1.SpeechContext();
        }
        SpeechContext.MergeFrom(other.SpeechContext);
      }
    }

    public void MergeFrom(pb::CodedInputStream input) {
      uint tag;
      while ((tag = input.ReadTag()) != 0) {
        switch(tag) {
          default:
            input.SkipLastField();
            break;
          case 8: {
            encoding_ = (global::Google.Cloud.Speech.V1.InitialRecognizeRequest.Types.AudioEncoding) input.ReadEnum();
            break;
          }
          case 16: {
            SampleRate = input.ReadInt32();
            break;
          }
          case 26: {
            LanguageCode = input.ReadString();
            break;
          }
          case 32: {
            MaxAlternatives = input.ReadInt32();
            break;
          }
          case 40: {
            ProfanityFilter = input.ReadBool();
            break;
          }
          case 48: {
            Continuous = input.ReadBool();
            break;
          }
          case 56: {
            InterimResults = input.ReadBool();
            break;
          }
          case 64: {
            EnableEndpointerEvents = input.ReadBool();
            break;
          }
          case 74: {
            OutputUri = input.ReadString();
            break;
          }
          case 82: {
            if (speechContext_ == null) {
              speechContext_ = new global::Google.Cloud.Speech.V1.SpeechContext();
            }
            input.ReadMessage(speechContext_);
            break;
          }
        }
      }
    }

    #region Nested types
    /// <summary>Container for nested types declared in the InitialRecognizeRequest message type.</summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute()]
    public static partial class Types {
      /// <summary>
      ///  Audio encoding of the data sent in the audio message. All encodings support
      ///  only 1 channel (mono) audio. Only `FLAC` includes a header that describes
      ///  the bytes of audio that follow the header. The other encodings are raw
      ///  audio bytes with no header.
      ///
      ///  For best results, the audio source should be captured and transmitted using
      ///  a lossless encoding (`FLAC` or `LINEAR16`). Recognition accuracy may be
      ///  reduced if lossy codecs (such as AMR, AMR_WB and MULAW) are used to capture
      ///  or transmit the audio, particularly if background noise is present.
      /// </summary>
      public enum AudioEncoding {
        /// <summary>
        ///  Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
        /// </summary>
        [pbr::OriginalName("ENCODING_UNSPECIFIED")] EncodingUnspecified = 0,
        /// <summary>
        ///  Uncompressed 16-bit signed little-endian samples.
        /// </summary>
        [pbr::OriginalName("LINEAR16")] Linear16 = 1,
        /// <summary>
        ///  This is the recommended encoding because it uses lossless compression;
        ///  therefore recognition accuracy is not compromised by a lossy codec.
        ///
        ///  The stream FLAC (Free Lossless Audio Codec) encoding is specified at:
        ///  http://flac.sourceforge.net/documentation.html.
        ///  Only 16-bit samples are supported.
        ///  Not all fields in STREAMINFO are supported.
        /// </summary>
        [pbr::OriginalName("FLAC")] Flac = 2,
        /// <summary>
        ///  8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
        /// </summary>
        [pbr::OriginalName("MULAW")] Mulaw = 3,
        /// <summary>
        ///  Adaptive Multi-Rate Narrowband codec. `sample_rate` must be 8000 Hz.
        /// </summary>
        [pbr::OriginalName("AMR")] Amr = 4,
        /// <summary>
        ///  Adaptive Multi-Rate Wideband codec. `sample_rate` must be 16000 Hz.
        /// </summary>
        [pbr::OriginalName("AMR_WB")] AmrWb = 5,
      }

    }
    #endregion

  }

  /// <summary>
  ///  Provides "hints" to the speech recognizer to favor specific words and phrases
  ///  in the results.
  /// </summary>
  [global::System.Diagnostics.DebuggerNonUserCodeAttribute()]
  public sealed partial class SpeechContext : pb::IMessage<SpeechContext> {
    private static readonly pb::MessageParser<SpeechContext> _parser = new pb::MessageParser<SpeechContext>(() => new SpeechContext());
    public static pb::MessageParser<SpeechContext> Parser { get { return _parser; } }

    public static pbr::MessageDescriptor Descriptor {
      get { return global::Google.Cloud.Speech.V1.CloudSpeechReflection.Descriptor.MessageTypes[2]; }
    }

    pbr::MessageDescriptor pb::IMessage.Descriptor {
      get { return Descriptor; }
    }

    public SpeechContext() {
      OnConstruction();
    }

    partial void OnConstruction();

    public SpeechContext(SpeechContext other) : this() {
      phrases_ = other.phrases_.Clone();
    }

    public SpeechContext Clone() {
      return new SpeechContext(this);
    }

    /// <summary>Field number for the "phrases" field.</summary>
    public const int PhrasesFieldNumber = 1;
    private static readonly pb::FieldCodec<string> _repeated_phrases_codec
        = pb::FieldCodec.ForString(10);
    private readonly pbc::RepeatedField<string> phrases_ = new pbc::RepeatedField<string>();
    /// <summary>
    ///  [Optional] A list of up to 50 phrases of up to 100 characters each to
    ///  provide words and phrases "hints" to the speech recognition so that it is
    ///  more likely to recognize them.
    /// </summary>
    public pbc::RepeatedField<string> Phrases {
      get { return phrases_; }
    }

    public override bool Equals(object other) {
      return Equals(other as SpeechContext);
    }

    public bool Equals(SpeechContext other) {
      if (ReferenceEquals(other, null)) {
        return false;
      }
      if (ReferenceEquals(other, this)) {
        return true;
      }
      if(!phrases_.Equals(other.phrases_)) return false;
      return true;
    }

    public override int GetHashCode() {
      int hash = 1;
      hash ^= phrases_.GetHashCode();
      return hash;
    }

    public override string ToString() {
      return pb::JsonFormatter.ToDiagnosticString(this);
    }

    public void WriteTo(pb::CodedOutputStream output) {
      phrases_.WriteTo(output, _repeated_phrases_codec);
    }

    public int CalculateSize() {
      int size = 0;
      size += phrases_.CalculateSize(_repeated_phrases_codec);
      return size;
    }

    public void MergeFrom(SpeechContext other) {
      if (other == null) {
        return;
      }
      phrases_.Add(other.phrases_);
    }

    public void MergeFrom(pb::CodedInputStream input) {
      uint tag;
      while ((tag = input.ReadTag()) != 0) {
        switch(tag) {
          default:
            input.SkipLastField();
            break;
          case 10: {
            phrases_.AddEntriesFrom(input, _repeated_phrases_codec);
            break;
          }
        }
      }
    }

  }

  /// <summary>
  ///  Contains audio data in the encoding specified in the
  ///  `InitialRecognizeRequest`. Either `content` or `uri` must be supplied.
  ///  Supplying both or neither returns [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
  /// </summary>
  [global::System.Diagnostics.DebuggerNonUserCodeAttribute()]
  public sealed partial class AudioRequest : pb::IMessage<AudioRequest> {
    private static readonly pb::MessageParser<AudioRequest> _parser = new pb::MessageParser<AudioRequest>(() => new AudioRequest());
    public static pb::MessageParser<AudioRequest> Parser { get { return _parser; } }

    public static pbr::MessageDescriptor Descriptor {
      get { return global::Google.Cloud.Speech.V1.CloudSpeechReflection.Descriptor.MessageTypes[3]; }
    }

    pbr::MessageDescriptor pb::IMessage.Descriptor {
      get { return Descriptor; }
    }

    public AudioRequest() {
      OnConstruction();
    }

    partial void OnConstruction();

    public AudioRequest(AudioRequest other) : this() {
      content_ = other.content_;
      uri_ = other.uri_;
    }

    public AudioRequest Clone() {
      return new AudioRequest(this);
    }

    /// <summary>Field number for the "content" field.</summary>
    public const int ContentFieldNumber = 1;
    private pb::ByteString content_ = pb::ByteString.Empty;
    /// <summary>
    ///  The audio data bytes encoded as specified in
    ///  `InitialRecognizeRequest`. Note: as with all bytes fields, protobuffers
    ///  use a pure binary representation, whereas JSON representations use base64.
    /// </summary>
    public pb::ByteString Content {
      get { return content_; }
      set {
        content_ = pb::ProtoPreconditions.CheckNotNull(value, "value");
      }
    }

    /// <summary>Field number for the "uri" field.</summary>
    public const int UriFieldNumber = 2;
    private string uri_ = "";
    /// <summary>
    ///  URI that points to a file that contains audio data bytes as specified in
    ///  `InitialRecognizeRequest`. Currently, only Google Cloud Storage URIs are
    ///  supported, which must be specified in the following format:
    ///  `gs://bucket_name/object_name` (other URI formats return
    ///  [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
    ///  [Request URIs](/storage/docs/reference-uris).
    /// </summary>
    public string Uri {
      get { return uri_; }
      set {
        uri_ = pb::ProtoPreconditions.CheckNotNull(value, "value");
      }
    }

    public override bool Equals(object other) {
      return Equals(other as AudioRequest);
    }

    public bool Equals(AudioRequest other) {
      if (ReferenceEquals(other, null)) {
        return false;
      }
      if (ReferenceEquals(other, this)) {
        return true;
      }
      if (Content != other.Content) return false;
      if (Uri != other.Uri) return false;
      return true;
    }

    public override int GetHashCode() {
      int hash = 1;
      if (Content.Length != 0) hash ^= Content.GetHashCode();
      if (Uri.Length != 0) hash ^= Uri.GetHashCode();
      return hash;
    }

    public override string ToString() {
      return pb::JsonFormatter.ToDiagnosticString(this);
    }

    public void WriteTo(pb::CodedOutputStream output) {
      if (Content.Length != 0) {
        output.WriteRawTag(10);
        output.WriteBytes(Content);
      }
      if (Uri.Length != 0) {
        output.WriteRawTag(18);
        output.WriteString(Uri);
      }
    }

    public int CalculateSize() {
      int size = 0;
      if (Content.Length != 0) {
        size += 1 + pb::CodedOutputStream.ComputeBytesSize(Content);
      }
      if (Uri.Length != 0) {
        size += 1 + pb::CodedOutputStream.ComputeStringSize(Uri);
      }
      return size;
    }

    public void MergeFrom(AudioRequest other) {
      if (other == null) {
        return;
      }
      if (other.Content.Length != 0) {
        Content = other.Content;
      }
      if (other.Uri.Length != 0) {
        Uri = other.Uri;
      }
    }

    public void MergeFrom(pb::CodedInputStream input) {
      uint tag;
      while ((tag = input.ReadTag()) != 0) {
        switch(tag) {
          default:
            input.SkipLastField();
            break;
          case 10: {
            Content = input.ReadBytes();
            break;
          }
          case 18: {
            Uri = input.ReadString();
            break;
          }
        }
      }
    }

  }

  /// <summary>
  ///  `NonStreamingRecognizeResponse` is the only message returned to the client by
  ///  `NonStreamingRecognize`. It contains the result as zero or more sequential
  ///  `RecognizeResponse` messages.
  ///
  ///  Note that streaming `Recognize` will also return multiple `RecognizeResponse`
  ///  messages, but each message is individually streamed.
  /// </summary>
  [global::System.Diagnostics.DebuggerNonUserCodeAttribute()]
  public sealed partial class NonStreamingRecognizeResponse : pb::IMessage<NonStreamingRecognizeResponse> {
    private static readonly pb::MessageParser<NonStreamingRecognizeResponse> _parser = new pb::MessageParser<NonStreamingRecognizeResponse>(() => new NonStreamingRecognizeResponse());
    public static pb::MessageParser<NonStreamingRecognizeResponse> Parser { get { return _parser; } }

    public static pbr::MessageDescriptor Descriptor {
      get { return global::Google.Cloud.Speech.V1.CloudSpeechReflection.Descriptor.MessageTypes[4]; }
    }

    pbr::MessageDescriptor pb::IMessage.Descriptor {
      get { return Descriptor; }
    }

    public NonStreamingRecognizeResponse() {
      OnConstruction();
    }

    partial void OnConstruction();

    public NonStreamingRecognizeResponse(NonStreamingRecognizeResponse other) : this() {
      responses_ = other.responses_.Clone();
    }

    public NonStreamingRecognizeResponse Clone() {
      return new NonStreamingRecognizeResponse(this);
    }

    /// <summary>Field number for the "responses" field.</summary>
    public const int ResponsesFieldNumber = 1;
    private static readonly pb::FieldCodec<global::Google.Cloud.Speech.V1.RecognizeResponse> _repeated_responses_codec
        = pb::FieldCodec.ForMessage(10, global::Google.Cloud.Speech.V1.RecognizeResponse.Parser);
    private readonly pbc::RepeatedField<global::Google.Cloud.Speech.V1.RecognizeResponse> responses_ = new pbc::RepeatedField<global::Google.Cloud.Speech.V1.RecognizeResponse>();
    /// <summary>
    ///  [Output-only] Sequential list of messages returned by the recognizer.
    /// </summary>
    public pbc::RepeatedField<global::Google.Cloud.Speech.V1.RecognizeResponse> Responses {
      get { return responses_; }
    }

    public override bool Equals(object other) {
      return Equals(other as NonStreamingRecognizeResponse);
    }

    public bool Equals(NonStreamingRecognizeResponse other) {
      if (ReferenceEquals(other, null)) {
        return false;
      }
      if (ReferenceEquals(other, this)) {
        return true;
      }
      if(!responses_.Equals(other.responses_)) return false;
      return true;
    }

    public override int GetHashCode() {
      int hash = 1;
      hash ^= responses_.GetHashCode();
      return hash;
    }

    public override string ToString() {
      return pb::JsonFormatter.ToDiagnosticString(this);
    }

    public void WriteTo(pb::CodedOutputStream output) {
      responses_.WriteTo(output, _repeated_responses_codec);
    }

    public int CalculateSize() {
      int size = 0;
      size += responses_.CalculateSize(_repeated_responses_codec);
      return size;
    }

    public void MergeFrom(NonStreamingRecognizeResponse other) {
      if (other == null) {
        return;
      }
      responses_.Add(other.responses_);
    }

    public void MergeFrom(pb::CodedInputStream input) {
      uint tag;
      while ((tag = input.ReadTag()) != 0) {
        switch(tag) {
          default:
            input.SkipLastField();
            break;
          case 10: {
            responses_.AddEntriesFrom(input, _repeated_responses_codec);
            break;
          }
        }
      }
    }

  }

  /// <summary>
  ///  `RecognizeResponse` is the only message type returned to the client.
  /// </summary>
  [global::System.Diagnostics.DebuggerNonUserCodeAttribute()]
  public sealed partial class RecognizeResponse : pb::IMessage<RecognizeResponse> {
    private static readonly pb::MessageParser<RecognizeResponse> _parser = new pb::MessageParser<RecognizeResponse>(() => new RecognizeResponse());
    public static pb::MessageParser<RecognizeResponse> Parser { get { return _parser; } }

    public static pbr::MessageDescriptor Descriptor {
      get { return global::Google.Cloud.Speech.V1.CloudSpeechReflection.Descriptor.MessageTypes[5]; }
    }

    pbr::MessageDescriptor pb::IMessage.Descriptor {
      get { return Descriptor; }
    }

    public RecognizeResponse() {
      OnConstruction();
    }

    partial void OnConstruction();

    public RecognizeResponse(RecognizeResponse other) : this() {
      Error = other.error_ != null ? other.Error.Clone() : null;
      results_ = other.results_.Clone();
      resultIndex_ = other.resultIndex_;
      endpoint_ = other.endpoint_;
    }

    public RecognizeResponse Clone() {
      return new RecognizeResponse(this);
    }

    /// <summary>Field number for the "error" field.</summary>
    public const int ErrorFieldNumber = 1;
    private global::Google.Rpc.Status error_;
    /// <summary>
    ///  [Output-only] If set, returns a [google.rpc.Status][] message that
    ///  specifies the error for the operation.
    /// </summary>
    public global::Google.Rpc.Status Error {
      get { return error_; }
      set {
        error_ = value;
      }
    }

    /// <summary>Field number for the "results" field.</summary>
    public const int ResultsFieldNumber = 2;
    private static readonly pb::FieldCodec<global::Google.Cloud.Speech.V1.SpeechRecognitionResult> _repeated_results_codec
        = pb::FieldCodec.ForMessage(18, global::Google.Cloud.Speech.V1.SpeechRecognitionResult.Parser);
    private readonly pbc::RepeatedField<global::Google.Cloud.Speech.V1.SpeechRecognitionResult> results_ = new pbc::RepeatedField<global::Google.Cloud.Speech.V1.SpeechRecognitionResult>();
    /// <summary>
    ///  [Output-only] For `continuous=false`, this repeated list contains zero or
    ///  one result that corresponds to all of the audio processed so far. For
    ///  `continuous=true`, this repeated list contains zero or more results that
    ///  correspond to consecutive portions of the audio being processed.
    ///  In both cases, contains zero or one `is_final=true` result (the newly
    ///  settled portion), followed by zero or more `is_final=false` results.
    /// </summary>
    public pbc::RepeatedField<global::Google.Cloud.Speech.V1.SpeechRecognitionResult> Results {
      get { return results_; }
    }

    /// <summary>Field number for the "result_index" field.</summary>
    public const int ResultIndexFieldNumber = 3;
    private int resultIndex_;
    /// <summary>
    ///  [Output-only] Indicates the lowest index in the `results` array that has
    ///  changed. The repeated `SpeechRecognitionResult` results overwrite past
    ///  results at this index and higher.
    /// </summary>
    public int ResultIndex {
      get { return resultIndex_; }
      set {
        resultIndex_ = value;
      }
    }

    /// <summary>Field number for the "endpoint" field.</summary>
    public const int EndpointFieldNumber = 4;
    private global::Google.Cloud.Speech.V1.RecognizeResponse.Types.EndpointerEvent endpoint_ = 0;
    /// <summary>
    ///  [Output-only] Indicates the type of endpointer event.
    /// </summary>
    public global::Google.Cloud.Speech.V1.RecognizeResponse.Types.EndpointerEvent Endpoint {
      get { return endpoint_; }
      set {
        endpoint_ = value;
      }
    }

    public override bool Equals(object other) {
      return Equals(other as RecognizeResponse);
    }

    public bool Equals(RecognizeResponse other) {
      if (ReferenceEquals(other, null)) {
        return false;
      }
      if (ReferenceEquals(other, this)) {
        return true;
      }
      if (!object.Equals(Error, other.Error)) return false;
      if(!results_.Equals(other.results_)) return false;
      if (ResultIndex != other.ResultIndex) return false;
      if (Endpoint != other.Endpoint) return false;
      return true;
    }

    public override int GetHashCode() {
      int hash = 1;
      if (error_ != null) hash ^= Error.GetHashCode();
      hash ^= results_.GetHashCode();
      if (ResultIndex != 0) hash ^= ResultIndex.GetHashCode();
      if (Endpoint != 0) hash ^= Endpoint.GetHashCode();
      return hash;
    }

    public override string ToString() {
      return pb::JsonFormatter.ToDiagnosticString(this);
    }

    public void WriteTo(pb::CodedOutputStream output) {
      if (error_ != null) {
        output.WriteRawTag(10);
        output.WriteMessage(Error);
      }
      results_.WriteTo(output, _repeated_results_codec);
      if (ResultIndex != 0) {
        output.WriteRawTag(24);
        output.WriteInt32(ResultIndex);
      }
      if (Endpoint != 0) {
        output.WriteRawTag(32);
        output.WriteEnum((int) Endpoint);
      }
    }

    public int CalculateSize() {
      int size = 0;
      if (error_ != null) {
        size += 1 + pb::CodedOutputStream.ComputeMessageSize(Error);
      }
      size += results_.CalculateSize(_repeated_results_codec);
      if (ResultIndex != 0) {
        size += 1 + pb::CodedOutputStream.ComputeInt32Size(ResultIndex);
      }
      if (Endpoint != 0) {
        size += 1 + pb::CodedOutputStream.ComputeEnumSize((int) Endpoint);
      }
      return size;
    }

    public void MergeFrom(RecognizeResponse other) {
      if (other == null) {
        return;
      }
      if (other.error_ != null) {
        if (error_ == null) {
          error_ = new global::Google.Rpc.Status();
        }
        Error.MergeFrom(other.Error);
      }
      results_.Add(other.results_);
      if (other.ResultIndex != 0) {
        ResultIndex = other.ResultIndex;
      }
      if (other.Endpoint != 0) {
        Endpoint = other.Endpoint;
      }
    }

    public void MergeFrom(pb::CodedInputStream input) {
      uint tag;
      while ((tag = input.ReadTag()) != 0) {
        switch(tag) {
          default:
            input.SkipLastField();
            break;
          case 10: {
            if (error_ == null) {
              error_ = new global::Google.Rpc.Status();
            }
            input.ReadMessage(error_);
            break;
          }
          case 18: {
            results_.AddEntriesFrom(input, _repeated_results_codec);
            break;
          }
          case 24: {
            ResultIndex = input.ReadInt32();
            break;
          }
          case 32: {
            endpoint_ = (global::Google.Cloud.Speech.V1.RecognizeResponse.Types.EndpointerEvent) input.ReadEnum();
            break;
          }
        }
      }
    }

    #region Nested types
    /// <summary>Container for nested types declared in the RecognizeResponse message type.</summary>
    [global::System.Diagnostics.DebuggerNonUserCodeAttribute()]
    public static partial class Types {
      /// <summary>
      ///  Indicates the type of endpointer event.
      /// </summary>
      public enum EndpointerEvent {
        /// <summary>
        ///  No endpointer event specified.
        /// </summary>
        [pbr::OriginalName("ENDPOINTER_EVENT_UNSPECIFIED")] Unspecified = 0,
        /// <summary>
        ///  Speech has been detected in the audio stream.
        /// </summary>
        [pbr::OriginalName("START_OF_SPEECH")] StartOfSpeech = 1,
        /// <summary>
        ///  Speech has ceased to be detected in the audio stream.
        /// </summary>
        [pbr::OriginalName("END_OF_SPEECH")] EndOfSpeech = 2,
        /// <summary>
        ///  The end of the audio stream has been reached. and it is being processed.
        /// </summary>
        [pbr::OriginalName("END_OF_AUDIO")] EndOfAudio = 3,
        /// <summary>
        ///  This event is only sent when continuous is `false`. It indicates that the
        ///  server has detected the end of the user's speech utterance and expects no
        ///  additional speech. Therefore, the server will not process additional
        ///  audio. The client should stop sending additional audio data.
        /// </summary>
        [pbr::OriginalName("END_OF_UTTERANCE")] EndOfUtterance = 4,
      }

    }
    #endregion

  }

  /// <summary>
  ///  A speech recognition result corresponding to a portion of the audio.
  /// </summary>
  [global::System.Diagnostics.DebuggerNonUserCodeAttribute()]
  public sealed partial class SpeechRecognitionResult : pb::IMessage<SpeechRecognitionResult> {
    private static readonly pb::MessageParser<SpeechRecognitionResult> _parser = new pb::MessageParser<SpeechRecognitionResult>(() => new SpeechRecognitionResult());
    public static pb::MessageParser<SpeechRecognitionResult> Parser { get { return _parser; } }

    public static pbr::MessageDescriptor Descriptor {
      get { return global::Google.Cloud.Speech.V1.CloudSpeechReflection.Descriptor.MessageTypes[6]; }
    }

    pbr::MessageDescriptor pb::IMessage.Descriptor {
      get { return Descriptor; }
    }

    public SpeechRecognitionResult() {
      OnConstruction();
    }

    partial void OnConstruction();

    public SpeechRecognitionResult(SpeechRecognitionResult other) : this() {
      alternatives_ = other.alternatives_.Clone();
      isFinal_ = other.isFinal_;
      stability_ = other.stability_;
    }

    public SpeechRecognitionResult Clone() {
      return new SpeechRecognitionResult(this);
    }

    /// <summary>Field number for the "alternatives" field.</summary>
    public const int AlternativesFieldNumber = 1;
    private static readonly pb::FieldCodec<global::Google.Cloud.Speech.V1.SpeechRecognitionAlternative> _repeated_alternatives_codec
        = pb::FieldCodec.ForMessage(10, global::Google.Cloud.Speech.V1.SpeechRecognitionAlternative.Parser);
    private readonly pbc::RepeatedField<global::Google.Cloud.Speech.V1.SpeechRecognitionAlternative> alternatives_ = new pbc::RepeatedField<global::Google.Cloud.Speech.V1.SpeechRecognitionAlternative>();
    /// <summary>
    ///  [Output-only] May contain one or more recognition hypotheses (up to the
    ///  maximum specified in `max_alternatives`).
    /// </summary>
    public pbc::RepeatedField<global::Google.Cloud.Speech.V1.SpeechRecognitionAlternative> Alternatives {
      get { return alternatives_; }
    }

    /// <summary>Field number for the "is_final" field.</summary>
    public const int IsFinalFieldNumber = 2;
    private bool isFinal_;
    /// <summary>
    ///  [Output-only] Set `true` if this is the final time the speech service will
    ///  return this particular `SpeechRecognitionResult`. If `false`, this
    ///  represents an interim result that may change.
    /// </summary>
    public bool IsFinal {
      get { return isFinal_; }
      set {
        isFinal_ = value;
      }
    }

    /// <summary>Field number for the "stability" field.</summary>
    public const int StabilityFieldNumber = 3;
    private float stability_;
    /// <summary>
    ///  [Output-only] An estimate of the probability that the recognizer will not
    ///  change its guess about this interim result. Values range from 0.0
    ///  (completely unstable) to 1.0 (completely stable). Note that this is not the
    ///  same as `confidence`, which estimates the probability that a recognition
    ///  result is correct.
    ///  This field is only provided for interim results (`is_final=false`).
    ///  The default of 0.0 is a sentinel value indicating stability was not set.
    /// </summary>
    public float Stability {
      get { return stability_; }
      set {
        stability_ = value;
      }
    }

    public override bool Equals(object other) {
      return Equals(other as SpeechRecognitionResult);
    }

    public bool Equals(SpeechRecognitionResult other) {
      if (ReferenceEquals(other, null)) {
        return false;
      }
      if (ReferenceEquals(other, this)) {
        return true;
      }
      if(!alternatives_.Equals(other.alternatives_)) return false;
      if (IsFinal != other.IsFinal) return false;
      if (Stability != other.Stability) return false;
      return true;
    }

    public override int GetHashCode() {
      int hash = 1;
      hash ^= alternatives_.GetHashCode();
      if (IsFinal != false) hash ^= IsFinal.GetHashCode();
      if (Stability != 0F) hash ^= Stability.GetHashCode();
      return hash;
    }

    public override string ToString() {
      return pb::JsonFormatter.ToDiagnosticString(this);
    }

    public void WriteTo(pb::CodedOutputStream output) {
      alternatives_.WriteTo(output, _repeated_alternatives_codec);
      if (IsFinal != false) {
        output.WriteRawTag(16);
        output.WriteBool(IsFinal);
      }
      if (Stability != 0F) {
        output.WriteRawTag(29);
        output.WriteFloat(Stability);
      }
    }

    public int CalculateSize() {
      int size = 0;
      size += alternatives_.CalculateSize(_repeated_alternatives_codec);
      if (IsFinal != false) {
        size += 1 + 1;
      }
      if (Stability != 0F) {
        size += 1 + 4;
      }
      return size;
    }

    public void MergeFrom(SpeechRecognitionResult other) {
      if (other == null) {
        return;
      }
      alternatives_.Add(other.alternatives_);
      if (other.IsFinal != false) {
        IsFinal = other.IsFinal;
      }
      if (other.Stability != 0F) {
        Stability = other.Stability;
      }
    }

    public void MergeFrom(pb::CodedInputStream input) {
      uint tag;
      while ((tag = input.ReadTag()) != 0) {
        switch(tag) {
          default:
            input.SkipLastField();
            break;
          case 10: {
            alternatives_.AddEntriesFrom(input, _repeated_alternatives_codec);
            break;
          }
          case 16: {
            IsFinal = input.ReadBool();
            break;
          }
          case 29: {
            Stability = input.ReadFloat();
            break;
          }
        }
      }
    }

  }

  /// <summary>
  ///  Alternative hypotheses (a.k.a. n-best list).
  /// </summary>
  [global::System.Diagnostics.DebuggerNonUserCodeAttribute()]
  public sealed partial class SpeechRecognitionAlternative : pb::IMessage<SpeechRecognitionAlternative> {
    private static readonly pb::MessageParser<SpeechRecognitionAlternative> _parser = new pb::MessageParser<SpeechRecognitionAlternative>(() => new SpeechRecognitionAlternative());
    public static pb::MessageParser<SpeechRecognitionAlternative> Parser { get { return _parser; } }

    public static pbr::MessageDescriptor Descriptor {
      get { return global::Google.Cloud.Speech.V1.CloudSpeechReflection.Descriptor.MessageTypes[7]; }
    }

    pbr::MessageDescriptor pb::IMessage.Descriptor {
      get { return Descriptor; }
    }

    public SpeechRecognitionAlternative() {
      OnConstruction();
    }

    partial void OnConstruction();

    public SpeechRecognitionAlternative(SpeechRecognitionAlternative other) : this() {
      transcript_ = other.transcript_;
      confidence_ = other.confidence_;
    }

    public SpeechRecognitionAlternative Clone() {
      return new SpeechRecognitionAlternative(this);
    }

    /// <summary>Field number for the "transcript" field.</summary>
    public const int TranscriptFieldNumber = 1;
    private string transcript_ = "";
    /// <summary>
    ///  [Output-only] Transcript text representing the words that the user spoke.
    /// </summary>
    public string Transcript {
      get { return transcript_; }
      set {
        transcript_ = pb::ProtoPreconditions.CheckNotNull(value, "value");
      }
    }

    /// <summary>Field number for the "confidence" field.</summary>
    public const int ConfidenceFieldNumber = 2;
    private float confidence_;
    /// <summary>
    ///  [Output-only] The confidence estimate between 0.0 and 1.0. A higher number
    ///  means the system is more confident that the recognition is correct.
    ///  This field is typically provided only for the top hypothesis. and only for
    ///  `is_final=true` results.
    ///  The default of 0.0 is a sentinel value indicating confidence was not set.
    /// </summary>
    public float Confidence {
      get { return confidence_; }
      set {
        confidence_ = value;
      }
    }

    public override bool Equals(object other) {
      return Equals(other as SpeechRecognitionAlternative);
    }

    public bool Equals(SpeechRecognitionAlternative other) {
      if (ReferenceEquals(other, null)) {
        return false;
      }
      if (ReferenceEquals(other, this)) {
        return true;
      }
      if (Transcript != other.Transcript) return false;
      if (Confidence != other.Confidence) return false;
      return true;
    }

    public override int GetHashCode() {
      int hash = 1;
      if (Transcript.Length != 0) hash ^= Transcript.GetHashCode();
      if (Confidence != 0F) hash ^= Confidence.GetHashCode();
      return hash;
    }

    public override string ToString() {
      return pb::JsonFormatter.ToDiagnosticString(this);
    }

    public void WriteTo(pb::CodedOutputStream output) {
      if (Transcript.Length != 0) {
        output.WriteRawTag(10);
        output.WriteString(Transcript);
      }
      if (Confidence != 0F) {
        output.WriteRawTag(21);
        output.WriteFloat(Confidence);
      }
    }

    public int CalculateSize() {
      int size = 0;
      if (Transcript.Length != 0) {
        size += 1 + pb::CodedOutputStream.ComputeStringSize(Transcript);
      }
      if (Confidence != 0F) {
        size += 1 + 4;
      }
      return size;
    }

    public void MergeFrom(SpeechRecognitionAlternative other) {
      if (other == null) {
        return;
      }
      if (other.Transcript.Length != 0) {
        Transcript = other.Transcript;
      }
      if (other.Confidence != 0F) {
        Confidence = other.Confidence;
      }
    }

    public void MergeFrom(pb::CodedInputStream input) {
      uint tag;
      while ((tag = input.ReadTag()) != 0) {
        switch(tag) {
          default:
            input.SkipLastField();
            break;
          case 10: {
            Transcript = input.ReadString();
            break;
          }
          case 21: {
            Confidence = input.ReadFloat();
            break;
          }
        }
      }
    }

  }

  #endregion

}

#endregion Designer generated code
